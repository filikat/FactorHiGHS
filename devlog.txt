*****
26 Feb 2024
I start implementing the analyze phase.
Most of the analyze phase can be taken from Tim Davis' book.
---
I need to choose which format of the matrix to accept as input, full or upper triangular.
I decide to accept both. 
If input is full, I extract the upper part.
If input is upper, nothing else is needed.
---
Function to permute the matrix symmetrically into upper triangular form is taken straight from Tim Davis.
I use this function also to extract the upper part from the full matrix, simply by using the identical permutation.
---
Function to get a permutation from Metis needs the full matrix, because Metis does not take triangular matrices for symmetric graphs.
If the original input was upper, I need to create a temporary copy of the full matrix. This is similar to Permute.
If the original input is full, I can save a pointer to it to use for Metis. Only that Metis takes non-const pointers.
I need to make a local copy of the full matrix to pass to Metis. Annoying, but not a big deal.
---
Function for the elimination tree is straightforward. 
The tree is saved in the vector parent, where if the parent is -1, then the node is the root.
The function in Tim Davis book works only for upper triangular matrices. I initially go with lower triangular, but I have to change the format to avoid over complicating things.
---
I need to implement the postorder, which should also be straightforward. 
But I am not sure if I should permute the matrix again. Maybe I can avoid it somehow.
In the future, the analyze phase should include also the ordering to reduce stack size for frontal matrices. For now, I ignore this.
---
I have two matlab functions to check that everything works on some small example. 
They convert between Matlab matrices and CSC matrices. 
After some debugging, everything runs smoothly.
---

*****
27 Feb 2024
I implemented the post ordering. 
Pretty straightforward, taken directly from Tim Davis.
At the moment, after computing the postorder, the elimination tree is relabeled and the matrix is permuted.
Also the vectors perm and iperm are updated, to keep track of the overall permutation applied to the original matrix.
I added some auxiliary functions to manage the permutations and moved them into a file of auxiliary functions.
---
I had some issued when trying with larger problems. I spotted some issues with the Matlab functions that I was using to generate the CSC matrix.
I fixed it and now it works for other problems as well.
I made the Matlab functions a bit more automatic, so that everything runs more easily and faster.
---
I am still not sure of what's the best input form.
As Julian pointed out, taking a full matrix is dangerous, so I may go back to taking only upper triangular.
Generating a local copy of the full matrix is not a big deal.
---

*****
28 Feb 2024
I spent a lot of time trying to use the algorithms in Tim Davis book for row and column counts. They don't work for some reason.
I will go back to that later. For now I am using the slightly inefficient approach.
It uses O(|L|) operations instead of O(|A|). Not a big deal, but it could be improved. 
For now everything works. I implemented the row and column counts and the full symbolic factorization.
Still missing supernode stuff.
I also need to decide how and what to store at the end. I may need a struct SymbolicFact to store the permutation, elimination tree and sparsity structure of L.
I am still trying to understand if I can avoid a second permutation with the postorder of the matrix. 
It should be possible, but I need to figure out some details. Not a big deal anyway. 
---
Some matrices, like pilot (but also others when using augmented system) give issues. 
There is a segfault during the call to Metis that I cannot understand.
I need to try and build the matrix directly in cpp and see if it persists. Maybe I am doing something wrong with the way I create the matrix in Matlab.
---

*****
29 Feb 2024
I implemented the fundamental supernodes. The algorithm to find them comes directly from Scott&Tuma book. 
For some matrices the supernodes are quite small, especially when using augmented system. It may be needed to merge them. For now I leave as it is.
Supernode information is stored as starting node of each supernode (fsn_ptr) and supernodal elimination tree.
An intermediate vector of length n is needed to find the tree efficiently.
---
I created a separate object Symbolic to store the information of the analyze phase. 
The idea is that an Analyze object creates a Symbollic object, and this is never modified again.
I need to adjust a couple of things because at the moment it is a bit of a mess.
No need to std::move everything if S is passed at the beginning and things are stored there directly.
Everything that is not needed after the analyze phase is discarded.
Symbolic is then accessed only by const public functions.
---
I implemented the relative indices for the frontal matrices, and also the extraction of the information of the clique tree.
There is an issue if the supernodes are merged, because the information about which rows are nonzeros in the frontal matrix cannot be read directly from L anymore.
The columns of the supernodes that were merged have to be merged as well and stored separately. 
I will think about this if and when we decide to merge supernodes.
---

*****
1 Mar 2024
Removed option to input full matrix.
Now, any matrix is given, it uses permute to extract the upper triangular part.
I decided to keep Analyze and Symbolic as they are, with std::move, instead of passing S at the beginning.
---
I changed test.cpp to directly read the mps file of the problem and build either the augmented system or the normal equations.
It can also presolve the problem. 
The Matlab test file now reads everything as input from file, instead of building the matrix separately.
---
For some problems (e.g.,pilot and woodw), Metis gives some weird errors, but only sometimes. The errors are not even always the same.
It gives segfault, bus error, trace trap. Backtracing in LLDB shows that this happens during the call to Metis.
The problem becomes less frequent reading the matrix directly from mps file and presolving (for pilot at least), but it persists for woodw.
No idea what is happening.
---
Also, the reordering using Metis is slightly worse than the reordering using dissect in Matlab.
It should use Metis underneath, but I cannot reproduce the same results by changing the options.
Maybe this only happens for small matrices, because amd is used at some point?
---
I improved the comments in Symbolic.h, so that everything is explained properly. 
I added a small example to show what the vectors contain.
---

*****
4 Mar 2024
I implemented the dsyrk BLAS function.
I basically copied it from the Netlib Fortran code and translated it to C.
This is the most basic implementation, not optimized much. 
It does access everything by columns, to get better data locality, but there's no other optimization.
Indeed, comparing it to the BLAS function already installed on the computer, there is a factor approximately 50 of difference.
There is basically no hope to compete with BLAS.
I think that some kind of divide-and-conquer may help, but we will never match or get close to the optimized BLAS.
I think we need to have the option to use pre-installed BLAS and Lapack.
---

*****
5 Mar 2024
I implemented dtrsm.
I improved the tests for the BLAS functions.
I can see that the time to solve the different subcases (there are 16 for dtrsm) changes considerably for different settings.
The transpose option seems to have a big effect. This is probably due to how the memory is accessed. 
However, trying multiple combinations and different sizes, the behaviour is still not too clear.
An important thing is that, to do the operations C = A*B, it is more efficient to store C and B by columns and A by rows.
If A fits in L1 cache, this is sometimes called data streaming, because the columns of C and B are streamed into cache one by one, while A is always present.
So, for example, doing A^T*A should be faster than doing A*A^T, if A is stored by columns.
I observe this, but if I reverse the size, the behaviour changes. There are probably multiple factors acting.
---

*****
6 Mar 2024
I moved the BLAS functions from cpp to c.
Now I can use restrict on the pointers of the matrices.
This does not bring any visible benenit though. Still, it's better to keep it.
---
I implemented dgemm.
I tried using loop unrolling on the functions, but it does not change the overall time.
I had a look at the disassembled object code, and simd instruction are already automatically used with -O3.
Cache optimization must be responsible for a large part of the performance difference compared to native BLAS.
I should try blocking the code, as in dpotrf2.
I finally understood how that works btw. I understood wrong the use of leading dimensions (lda,ldb,ldc) in BLAS.
I updated the access to the arrays to use lda and blocking the code should be straightoforward.
I wonder if using indexes like dpotrf2 is actually the best thing for cache efficiency though, especially if the columns of the matrix are not aligned with the cache lines.
Copying the matrix in a smaller array would surely make the data more compact and thus improve cache efficiency, even though one has to copy everything.
I should try both approaches.
---
I tried splitting the matrix as 2x2 blocks of the same size. 
I then applied recursion until a base case is reached (where sizes are smaller than 32).
For dgemm, this bring a benefit for some cases (transa is 'T'), but makes it worse for others (transa is 'N').
---
Another important thing to notice is that, to achieve good cache efficiency, the columns of the matrix should be aligned to the cache lines.
This does not happen unless the size of the marix is a multiple of the cache line size (128B for my mac, or 16 double).
It may be useful to copy the data into local storage and add padding until the next multiple of 128B.
I am not sure how much this affects the performance, but I should try the approach with local copies.
---

*****
7-8-9 Mar 2024
I went into the rabbit hole of understanding how the optimized BLAS are implemented. 
It's very complicated and only makes sense if one writes assembly code.
I tried to reproduce the technique described by Goto, that should be used in openblas, but it doesn't really give an advantage.
---
I think it's better to stop focusing on an implementation of BLAS for now and continue with the factorization.
We will worry about which BLAS to use later on.
However, I do need to implement the partial dportf, to do partial factorization of the frontal matrices.
I also need to check if implementing dpotrf using native BLAS gives the same results as using the native Lapack.
---

*****
11 Mar 2024
I implemented dpotrf and dpotrf2.
A bit surprisingly, the C implementations that call level 2 and level 3 BLAS are faster than the native lapack library.
The advantage is around 20%, sometimes a bit more.
It's a it weird, because the performance of Lapack should come completely from BLAS, so using the same BLAS should yield the same times.
Anyway, this is a good sign, because it means that I can implement the partial dense factorization, and add regularizaiton, without penalizing the performance.
If my dpotrf was substantially slower than the native one, it would be a problem to use my own version with regularization.
I now want to try to see how much faster the packed version is.
---
Implementing the packed version has some tricky parts.
I had to find a way to pack the matrix. I will have to undo this at the end.
I had to understand how to use the BLAS functions on the packed matrices. Everything is transpose.
I has to make a local unpacked copy of the diagonal blocks and un-unpack it.
Something is not working still.
I will have to go through each step with a small matrix to see what's happening.
---

*****
12 Mar 2024
The packed version now works.
It is faster than both Lapack and the version that I made of dpotrf. However, the packing and unpacking operations can take a long time.
For now, I will ignore the packed version and use the normal one. 
In the future, I need to understand if the matrices can be produced already packed and if the forward/backsolves can be done in the packed format.
This would bring a decent advantage for large dense matrices.
---
I need to make all these code do the partial factorization.
I tried using a right looking approach in dpotrf, but this slows down the code quite a bit.
I wonder if it is faster to use a left looking approach, and at the end update the Schur complement just one.
---
Yes, using the left looking is quite faster, by a factor 3-3.5 for large matrices.
I tried to compute the number of operations that are required when doing a partial factorization with respect to the full factorization:
if only a fraction p\in[0,1] of the columns are eliminated, out of n, then the number of operations is asymptotic to
(1/3*n^3) * p*(p^2-3p+3)
where the first part (1/3*n^3) is the cost of the full factorization and p*(p^2-3p+3) is the reduction coefficient.
The times that I measure using the left and right looking approach seem to more or less follow this rule.
---

*****
13 Mar 2024
I realized that the left-looking partial factorization can be implemented using two separate arrays for the first k columns and the remaining Schur complement.
This is because the update of the Schur complement is done once at the very end, with a single BLAS call to dsyrk.
One can still use the same array for both, simply providing the correct starting location of the Schur complement block and the correct leading dimension.
I can still use restrict, because there is no aliasing happening between the two arrays.
Doing like this is useful in managing the frontal matrices.
The first k columns will be stored until the end, or copied and immediately destroyed. 
The remaining Schur complement will be kept until the parent is considered and then destroyed.
The two blocks have different lifespan within the code.
Storing them in separate arrays should make it easier to deal with this.
---

*****
14 Mar 2024
I added a function to check that the symbolic factorization is correct.
It creates a dense version of the original matrix and factorizes it using Lapack, with random entries.
It then checks that the nonzeros happen where the symbolic factorization predicts.
With random entries, numerical cancellation is highly unlikely.
If the matrix is too large, the check is not done.
---

*****
18 Mar 2024
I think it's better to avoid overcomplicationg the analyze phase for now.
I will keep the double permutation of the matrix for now.
I will keep the slow row and col counts, without using the skeleton matrix.
If needed, I will come back to these later.
---
Added computation of number of operations required, as the sum of the square of the number of nonzero entries in each column.
Added the print of the summary of the symbolic object.
---

*****
19 Mar 2024
I moved the functions that do the partial factorization into a single file PartialFact.
I also changed them so that they can do both Cholesky and LDL. 
The problem with LDL is that there are no BLAS functions with intermediate diagonal scaling.
To compute A*D*A', for a generic A and a diagonal D: 
- if D is spd, then A*D*A' = (A * D^1/2) * (D^1/2 * A') and dsyrk can be used;
- if D is generic, then I have to make a local copy of A, multiply it with D and use dgemm.
This is slower because it requires intermediate copies, but also because dgemm fills the full matrix, rather than just one of the triangles, like dsyrk.
Indeed, there is almost a factor 2 of difference when using the indefinite code instead of the positive definite.
dgemm may be more optimized than dsyrk, but it still does a more complicated operation.
I need to figure out if there is a faster way of dealing with the indefinite case.
---
To make the copies, I used dcopy.
I read in some forum that this may be less efficient than memcpy sometimes. However, I expect dcopy to be highly optimized, so it shouldn't make any difference if it is properly implemented.
I will stick to dcopy for now.
---
I should also use blocks when building the Schur complement remaining after the partial factorization.
This would use more BLAS calls, but less memory, as only a smaller copy would be needed.
However, this may be considerably slower, similarly to how the right looking approach was.
---

*****
20 Mar 2024
I did a time profile of the positive definite and indefinite dense factorizations, to see where the difference is, for a matrix of size 20000, with 10% of columns eliminated.

     Time profile     Indefinite        Pos-definite          Ratio
        Time copy       0.004962            0.000000            inf
 Time diag update       0.002041            0.002848           0.72
   Time diag fact       0.000515            0.000298           1.73
 Time update cols       0.373991            0.389060           0.96
  Time schur copy       0.043392            0.000000            inf
Time update schur       3.969590            1.965485           2.02

The copies required by the indefinite code do not take too much time.
Updating the diagonal blocks take the same time. Factorizing the diagonal blocks takes twice as much, but this is a very small number anyway.
Updating the block of columns takes the same time.
Most of the time lost is in updating the Schur complement.
I have two ideas to try:
- using blocks when updating the Schur complement
- splitting the copies between positive and negative pivots, scaling the columns with sqrt(|Ajj|) and using dsyrk, i.e.,
  the product A * D * A^T, where D is diagonal with 1 or -1 on the diagonal, can be written as
    \sum_i D_i A_i A_i^T = \sum_{i: Di=1} A_iA_i^T - \sum_{i: Di=-1} A_iA_i^T.
  In this way, I can use 2 calle to dsyrk instead of a call to dgemm.
I think that the factor 2 that arises when updating the Schur complement comes from the fact that dgemm updates the full matrix and not just the triangle.
Using dsyrk should solve this, taking some extra time for the copies.
---
Indeed, splitting into positive and negative and using dsyrk solves the problem. 

     Time profile     Indefinite        Pos-definite          Ratio
        Time copy       0.005025            0.000000            inf
 Time diag update       0.002093            0.002313           0.90
   Time diag fact       0.000510            0.000297           1.72
 Time update cols       0.376947            0.377971           1.00
  Time schur copy       0.035899            0.000000            inf
Time update schur       1.982756            1.980476           1.00

The update of the Schur complement now takes the same time.
The copies can be expensive, if many columns are eliminated (k=n or close).
Hopefully, this is not the case with the frontal matrices, as I expect only a small number of columns to be eliminated.
The update of the diagonal block is cheaper in the indefinite case probably because dgemm is better optimized than dsyrk, and the block is small.
I don't see a point in optimizing more the code for the factorization of the blocks. It takes twice as much, but this time is negligible.
---

*****
21 Mar 2024
I noticed that I also need relative indices to to assemble the original columns into the frontal matrix.
I now have relind_cols for that and relind_clique to assemble the generated elements.
In doing so, I apparently broke something and things were not working properly.
In particular, Metis was giving the usual problems, but worse.
I finally noticed that Metis assumes that the adjacency list of each vertex does not contain the vertex itself...
This means that I should not consider diagonal entries when building the full matrix for Metis.
I fixed this and now everything seems to work properly.
The old segfault and bus error and whatnot were all due to this. What an idiot.
---
I also noticed an error in the way the supernodes are detected. It has to do with Fortran and C indices starting from 1 and 0. 
It should be fixed now.
The issue of the first node not being detected correctly also disappeared.
---
I added a check for the relative indices of the columns.
I don't think this makes much sense, because the only way to check is to almost do the same operations as when I compute them.
I leave it there for now anyway.
---

*****
22 Mar 2024
I tested the analyze phase on the whole Netlib collection.
It works for all problems, both augmented system and normal equations.
I spotted another bug that was giving problems: I was using fsn_parent when it was -1. 
I have to remember to always check for roots when using the elimination tree.
The total times for the analyze phase only are:
normal equations, 0.49s
augmented system, 0.84s
---

*****
28 Mar 2024
I added a double transpose to sort the columns.
It's needed to assemble more efficiently the frontal matrices.
---
I started implementing the factorize phase.
It's a bit of a mess because Analyze takes the upper part and Factorize takes the lower part.
I may need to change the input to analyze.
---
The factorize phase was surprisingly easy to get working.
I'm not sure if everything is exactly correct, I still need to implement a proper check, but for now it seems to work.
---

*****
29 Mar 2024
I changed the frontal matrix from std::vector<double> to double*. 
It gives me a better control over the allocation and deallocation of memory.
There is also no need of any more complicated functionality, so no need for the vector overhead.
Schur contribution of children is now freed after being used.
---
I changed the input to Analyze to be lower triangular, so that both analyze and factorize phases take as input the lower part.
I am not sure if the columns need to be sorted at the beginning of Analyze. They surely need to be when I compute the relative indices using the lower part, but maybe I can skip it for the upper part.
Anyway, for now the columns are sorted with a double transpose.
---
I also changed the input to the constructor to take std::vector, so that I can skip an initial copy.
The code is not meant to be general purpose anyway, so taking the pointer as input does not make much sense.
---
I added a proper Check function.
It computes the factorization using Lapack dpotrf and computes the Frobenius norm of the error, divided by the Frobenius norm of the Lapack factor.
I set a threshold of 1e-12 and is seems to work for all problems.
Interesting that Matlab gives a slightly different (worse) error. Probably it uses Cholmod, that is supernodal, and it sums contributions in a different order.
---
I did some time profiling. qap15 is large enough to see where the time is lost (as usual I initially forgot to switch to -O3...)
Most of the time is lost in assemblying the frontal matrix, rather than factorizing it.
Quite a bit of time is used to initialize Frontal and Clique to zero. Even changing to std::fill or even memset, the time remains high.
Very little time is used to assemble the original matrix A.
Most of the time is used for the children supernodes.
This is 3-4 times the time taken by the dense factorization.
---
It actually is the assembly of the children into Clique that is slow, because Clique is usually larger and has more contributions than Frontal.
It is interesting that the time for dense factorization is smaller (roughly half with qap15) than the time to initialize to zero the frontal matrix.
---

*****
1 Apr 2024
The assemble of children into the frontal matrix now exploits consecutive indices and daxpy.
This reduces considerably the time to assemble the children (by 50% for qap15).
It requires some more memory and some more preliminary computation in Analyze.
---
I improved the time profile of both Analyze and Factorize, and the way in which it is printed.
---

*****
2 Apr 2024
I implemented the merging of supernodes.
I create a new permutation (because children of a supernode may be shuffled), I update fsn_start and fsn_parent.
I need to understand how to merge the columns of the supernode to obtain a representative column.
I also need to perform the symbolic factorization only on supernodes and not on all the columns.
---
I think I managed to generated the sparsity pattern only for the supernode.
I also generated the relative indices and they seem to be right (still have to do a proper check).
---

*****
3 Apr 2024
I adjusted Factorize to use the supernodal pattern.
A little bit of trouble with Check, but I figured it out.
I did some tests on qap15 with different values of max_artificial_nz.
The assembly time becomes considerably smaller. 
The factorize time also becomes smaller, I guess because it does fewer calls to BLAS and each call involves a larger matrix, and this is more efficient.
The prepare also becomes smaller, I guess because even if it needs to allocate and initialize a slightly larger portion of memory, it comes into big continuous chunks rather that small ones.

max_art_nz    sn found    Prepare     Assembly clique     dense factorization
    0           1643       0.50           0.51                    0.37
   16           1399       0.30           0.33                    0.30
   64            759       0.21           0.23                    0.25
  128            581       0.17           0.20                    0.22
  256            421       0.14           0.18                    0.19

Quite impressive results!
It took a lot of effort to implement the relaxed supernodes, but it makes a gigantic difference.
---
Great revelation!!
I don't need to initialize Clique to zero!
I have to initialize Frontal to zero and assemble the contributions there, because they are needed for the partial factorization.
But the contributions to Clique can be summed later, provided that the partial factorization ignores the previous content of Clique (just use beta = 0 in call to dsyrk).
This reduces the prepare time by almost two orders of magnitude, to about 0.005 for qap15.
---
I slightly changed how Symbolic is accessed, to have a less cumbersone code.
---

*****
4 Apr 2024
Changed the way that the Netlib tests are done.
For some reason, the previous way was giving strange errors.
Now the results are printed to an external files, so that it is easy to compare how much time is used for each of the phases.
---
Using -fsanitize=address I found a buffer overflow that I didn't notice. Of course, it was the usual problem the parent being -1.
It also finds some memory leaks, but I cannot figure out where.
One looks to be inside of metis and its size is fixed, it does not depend on the problem.
The others are puzzling, but very small.
---
I plotted the time for each supernode, compared to the supernode size and the frontal matrix size.
There is clearly a correlation with the supernode size. But the times are very clearly correlated with the frontal matrix size. Nothing too surprosing really.
I still have to understand if I should impose a maximum size to the supernodes. This could easily be done by splitting the large ones in FundamentalSupernodes, right after they are found, and then not merging them if they would produce a large supernode.
---
I need to implement the solve phase, which should be easy enough.
Then I need to compare the result with MA86.
I am curios to see how the speed compares.
---

*****
5 Apr 2024
I implemented the solve phase.
Quite easy, no big deal.
---
I included MA86.
Some issues to make it work.
MC68 is giving some strange error, but for now I can ignore it and use the same permutation for both codes.
After linking openmp properly, everything works.
---
I tried on random right-hand sides and the relative error is around 1e-16.
There was an issue with the fact that MA86 accepts only the lower triangle. It does not extract it.
This may have beed the reason Matlab was giving a larger error as well.
No need to check with Matlab anymore now. I use it just for plotting.
The times are weird. For some problems my (serial) code is faster than MA86.
But MA86 uses 8 cores, while mine only 1.
Something is strange. I need to try with MA57, which is multifrontal.
---

*****
8 Apr 2024
I tried different techniques to merge supernodes. 
I would like to find a way that does not depent on a specific parameter that may need to be adjusted for a given matrix.
I tried to merge based on the percentage of fake nonzeros out of the total number of nonzeros in the merged supernode.
I tried also based on the percentage of extra operations required wrt the operations required without relaxation.
I have an idea ot trying whatever strategy iteratively: try once, if extra operations are less than 1% of the total operations, try again with higher threshold.
Continue until you fall within 1% and 2%. If you go over 2%, then reject and try with smaller threshold.
Something like this.
I also think that it is better to choose what to merge based on number of extra operations rather than number of extra nonzeros.
---
It may be useful to merge supernodes if they are too small.
I added a rule to merge a parent with its smallest child, if both are smaller than a threshold.
This is what is used in the HSL codes. However, it does create more fake nonzeros.
I am not sure of whether I should keep the basic strategies (together maybe), or try to find a better one.
---
I understood why MA86 is slower. It does pivoting. I should try with MA87.
Then, MA97 is multifrontal rather than supernodal and I can set it so that it does not do pivoting.
I should include both of these in my comparison.
---

*****
9 Apr 2024
I slightly changed the functions to merge supernodes, so that it is easier to try new strategies.
I managed to get the skeleton matrix for the column counts to work.
It was taking quite a bit of time for large matrices, now it takes a very small amount.
I slightly changed how Symbolic prints the summary.
---

*****
10 Apr 2024
I included ma87 and ma97. I also requested ma57.
They still seem slow. The relative error is small, but maybe I should try with more ill conditioned theta.
---

*****
11 Apr 2024
I included ma57.
It looks quite a bit slower than the others.
The documentation also says that it is not parallel.
---
If I use an amd ordering for the Netlib problems, the time of ma86 is the best, twice faster than my code (run on a single thread).
Also for larger, problems, the hsl codes seem to go better with amd from mc68. Strange, because if Metis 4 was available they would use it to find a nested dissection.
If I use Metis ordering for all of them, my code remains the fastest.
At least until I found the next thing that I am doing wrong...
---

*****
12 Apr 2024
I run some tests forcing all code to use Metis or amd (apart from ma57, which has no option to receive an ordering from the user, but it still find an amd ordering internally).
The results are weird.
First of all, the hsl codes modify the ordering that I provide in some way during analyse.
This modification sometimes seems to break things.
In particular if I provide the Metis ordering, things go very weird.
The number of nonzeros returned in info is also different from the one that I can observe in Matlab if I use the same (modified) ordering.
---
An example with nug08 (all run in serial, pivoting switch off):

             |------------- AMD ------------| |------------ Metis -----------|  
code          factor time       reported nnz|  factor time       reported nnz
ProtoFact         30                1.5e8   |       2                4.5e7
MA86               8                1.0e8   |      20                1.5e8
MA87               6.5              1.0e8   |      19                1.5e8
MA97              16                1.0e8   |      99                1.5e8
MA57              30                  -     |      30                  -

The behaviour seems to be opposite: with metis my code is very fast and hsl is very slow. With amd my code is very slow and hsl is fast.
However, the best time is from my code with Metis. Probably because it has a smaller number of nonzeros.
Indeed, all this strange behaviour could be related only to the number of nonzeros and number of operations performed.
It would be a good thing, because it would mean that my code is on par with hsl (on 1 thread) and the difference is made only by which ordering is used.
Maybe I should install Metis 4 to see if it makes a difference.
---
I am not sure if I understood how Jennifer merges supernodes. If I just merge child-parent when they are too small, there are a huge number of fake nonzeros that are created.
It seems better to go with Ashcraft-Grimes type of merging.
Especially when using the augmented system structure, where there are fewer true supernodes, merging must be done carefully or there are plenty of fake nonzeros added.
---
I slightly changed the second criterion to merge supernodes.
It used to merge the smallest child with the parent, if they were both below a threshold.
Now, among all the children below the threshold, it select the one which produces the smallest number of nonzeros, and merges it. 
---

*****
15 Apr 2024
I found a bug when trying with a larger matrix.
Something to do with the way the indices for the supernodes were counted.
Indeed, I was using a pretty stupid method. Now it's fixed.
---
I found the reason why the ordering seemed so strange.
MC68 returns the inverse permutation and MAxx takes as input the inverse permutation.
What I was providing through Metis was the direct permutation.
Quick fix and now the comparison between the codes, with the two orderings, makes more sense.
---

*****
24 Apr 2024
Back from holiday.
Minor changes to the printing.
Added printing of times for each Blas function.
---
Changed some names of variables to have a more uniform naming convention.
I am aiming at using this convention:
 - CapitalNames for functions and classes
 - thisStyle for class variables
 - ThisStyle for class functions
 - this_style for local variables and other temporary stuff
 - k_this_style for constants
---
Changed constructor of Factorise so that it takes std::vector as input. No idea why it was still using pointers.
---
Small bug when computing the density, n * n was overflowing for large matrix.
---

*****
29 Apr 2024
I changed the solve routines to use BLAS 2.
I moved all the Blas declarations in a single file.
Eventually, the columns may need to be stored in packed format. 
I am not sure how to do the forward and back solve in packed format, since tpsv does not have a 'leading dimension' argument.
Probably the hybrid storage format allows to do something clever about that.
---

*****
To do:
- postorder of supernodal tree, to optimize stack size
- switch to clique stored in packed format:
  . assembly into clique uses packed format indexing
  . clique is packed after partial factorization