*****
26 Feb 2024
I start implementing the analyze phase.
Most of the analyze phase can be taken from Tim Davis' book.
---
I need to choose which format of the matrix to accept as input, full or upper triangular.
I decide to accept both. 
If input is full, I extract the upper part.
If input is upper, nothing else is needed.
---
Function to permute the matrix symmetrically into upper triangular form is taken straight from Tim Davis.
I use this function also to extract the upper part from the full matrix, simply by using the identical permutation.
---
Function to get a permutation from Metis needs the full matrix, because Metis does not take triangular matrices for symmetric graphs.
If the original input was upper, I need to create a temporary copy of the full matrix. This is similar to Permute.
If the original input is full, I can save a pointer to it to use for Metis. Only that Metis takes non-const pointers.
I need to make a local copy of the full matrix to pass to Metis. Annoying, but not a big deal.
---
Function for the elimination tree is straightforward. 
The tree is saved in the vector parent, where if the parent is -1, then the node is the root.
The function in Tim Davis book works only for upper triangular matrices. I initially go with lower triangular, but I have to change the format to avoid over complicating things.
---
I need to implement the postorder, which should also be straightforward. 
But I am not sure if I should permute the matrix again. Maybe I can avoid it somehow.
In the future, the analyze phase should include also the ordering to reduce stack size for frontal matrices. For now, I ignore this.
---
I have two matlab functions to check that everything works on some small example. 
They convert between Matlab matrices and CSC matrices. 
After some debugging, everything runs smoothly.
---

*****
27 Feb 2024
I implemented the post ordering. 
Pretty straightforward, taken directly from Tim Davis.
At the moment, after computing the postorder, the elimination tree is relabeled and the matrix is permuted.
Also the vectors perm and iperm are updated, to keep track of the overall permutation applied to the original matrix.
I added some auxiliary functions to manage the permutations and moved them into a file of auxiliary functions.
---
I had some issued when trying with larger problems. I spotted some issues with the Matlab functions that I was using to generate the CSC matrix.
I fixed it and now it works for other problems as well.
I made the Matlab functions a bit more automatic, so that everything runs more easily and faster.
---
I am still not sure of what's the best input form.
As Julian pointed out, taking a full matrix is dangerous, so I may go back to taking only upper triangular.
Generating a local copy of the full matrix is not a big deal.
---

*****
28 Feb 2024
I spent a lot of time trying to use the algorithms in Tim Davis book for row and column counts. They don't work for some reason.
I will go back to that later. For now I am using the slightly inefficient approach.
It uses O(|L|) operations instead of O(|A|). Not a big deal, but it could be improved. 
For now everything works. I implemented the row and column counts and the full symbolic factorization.
Still missing supernode stuff.
I also need to decide how and what to store at the end. I may need a struct SymbolicFact to store the permutation, elimination tree and sparsity structure of L.
I am still trying to understand if I can avoid a second permutation with the postorder of the matrix. 
It should be possible, but I need to figure out some details. Not a big deal anyway. 
---
Some matrices, like pilot (but also others when using augmented system) give issues. 
There is a segfault during the call to Metis that I cannot understand.
I need to try and build the matrix directly in cpp and see if it persists. Maybe I am doing something wrong with the way I create the matrix in Matlab.
---

*****
29 Feb 2024
I implemented the fundamental supernodes. The algorithm to find them comes directly from Scott&Tuma book. 
For some matrices the supernodes are quite small, especially when using augmented system. It may be needed to merge them. For now I leave as it is.
Supernode information is stored as starting node of each supernode (fsn_ptr) and supernodal elimination tree.
An intermediate vector of length n is needed to find the tree efficiently.
---
I created a separate object Symbolic to store the information of the analyze phase. 
The idea is that an Analyze object creates a Symbollic object, and this is never modified again.
I need to adjust a couple of things because at the moment it is a bit of a mess.
No need to std::move everything if S is passed at the beginning and things are stored there directly.
Everything that is not needed after the analyze phase is discarded.
Symbolic is then accessed only by const public functions.
---
I implemented the relative indices for the frontal matrices, and also the extraction of the information of the clique tree.
There is an issue if the supernodes are merged, because the information about which rows are nonzeros in the frontal matrix cannot be read directly from L anymore.
The columns of the supernodes that were merged have to be merged as well and stored separately. 
I will think about this if and when we decide to merge supernodes.
---

*****
1 Mar 2024
Removed option to input full matrix.
Now, any matrix is given, it uses permute to extract the upper triangular part.
I decided to keep Analyze and Symbolic as they are, with std::move, instead of passing S at the beginning.
---
I changed test.cpp to directly read the mps file of the problem and build either the augmented system or the normal equations.
It can also presolve the problem. 
The Matlab test file now reads everything as input from file, instead of building the matrix separately.
---
For some problems (e.g.,pilot and woodw), Metis gives some weird errors, but only sometimes. The errors are not even always the same.
It gives segfault, bus error, trace trap. Backtracing in LLDB shows that this happens during the call to Metis.
The problem becomes less frequent reading the matrix directly from mps file and presolving (for pilot at least), but it persists for woodw.
No idea what is happening.
---
Also, the reordering using Metis is slightly worse than the reordering using dissect in Matlab.
It should use Metis underneath, but I cannot reproduce the same results by changing the options.
Maybe this only happens for small matrices, because amd is used at some point?
---
I improved the comments in Symbolic.h, so that everything is explained properly. 
I added a small example to show what the vectors contain.
---

*****
4 Mar 2024
I implemented the dsyrk BLAS function.
I basically copied it from the Netlib Fortran code and translated it to C.
This is the most basic implementation, not optimized much. 
It does access everything by columns, to get better data locality, but there's no other optimization.
Indeed, comparing it to the BLAS function already installed on the computer, there is a factor approximately 50 of difference.
There is basically no hope to compete with BLAS.
I think that some kind of divide-and-conquer may help, but we will never match or get close to the optimized BLAS.
I think we need to have the option to use pre-installed BLAS and Lapack.
---

*****
5 Mar 2024
I implemented dtrsm.
I improved the tests for the BLAS functions.
I can see that the time to solve the different subcases (there are 16 for dtrsm) changes considerably for different settings.
The transpose option seems to have a big effect. This is probably due to how the memory is accessed. 
However, trying multiple combinations and different sizes, the behaviour is still not too clear.
An important thing is that, to do the operations C = A*B, it is more efficient to store C and B by columns and A by rows.
If A fits in L1 cache, this is sometimes called data streaming, because the columns of C and B are streamed into cache one by one, while A is always present.
So, for example, doing A^T*A should be faster than doing A*A^T, if A is stored by columns.
I observe this, but if I reverse the size, the behaviour changes. There are probably multiple factors acting.
---

*****
6 Mar 2024
I moved the BLAS functions from cpp to c.
Now I can use restrict on the pointers of the matrices.
This does not bring any visible benenit though. Still, it's better to keep it.
---
I implemented dgemm.
I tried using loop unrolling on the functions, but it does not change the overall time.
I had a look at the disassembled object code, and simd instruction are already automatically used with -O3.
Cache optimization must be responsible for a large part of the performance difference compared to native BLAS.
I should try blocking the code, as in dpotrf2.
I finally understood how that works btw. I understood wrong the use of leading dimensions (lda,ldb,ldc) in BLAS.
I updated the access to the arrays to use lda and blocking the code should be straightoforward.
I wonder if using indexes like dpotrf2 is actually the best thing for cache efficiency though, especially if the columns of the matrix are not aligned with the cache lines.
Copying the matrix in a smaller array would surely make the data more compact and thus improve cache efficiency, even though one has to copy everything.
I should try both approaches.
---
I tried splitting the matrix as 2x2 blocks of the same size. 
I then applied recursion until a base case is reached (where sizes are smaller than 32).
For dgemm, this bring a benefit for some cases (transa is 'T'), but makes it worse for others (transa is 'N').
---
Another important thing to notice is that, to achieve good cache efficiency, the columns of the matrix should be aligned to the cache lines.
This does not happen unless the size of the marix is a multiple of the cache line size (128B for my mac, or 16 double).
It may be useful to copy the data into local storage and add padding until the next multiple of 128B.
I am not sure how much this affects the performance, but I should try the approach with local copies.
---

*****
7-8-9 Mar 2024
I went into the rabbit hole of understanding how the optimized BLAS are implemented. 
It's very complicated and only makes sense if one writes assembly code.
I tried to reproduce the technique described by Goto, that should be used in openblas, but it doesn't really give an advantage.
---
I think it's better to stop focusing on an implementation of BLAS for now and continue with the factorization.
We will worry about which BLAS to use later on.
However, I do need to implement the partial dportf, to do partial factorization of the frontal matrices.
I also need to check if implementing dpotrf using native BLAS gives the same results as using the native Lapack.
---

*****
11 Mar 2024
I implemented dpotrf and dpotrf2.
A bit surprisingly, the C implementations that call level 2 and level 3 BLAS are faster than the native lapack library.
The advantage is around 20%, sometimes a bit more.
It's a it weird, because the performance of Lapack should come completely from BLAS, so using the same BLAS should yield the same times.
Anyway, this is a good sign, because it means that I can implement the partial dense factorization, and add regularizaiton, without penalizing the performance.
If my dpotrf was substantially slower than the native one, it would be a problem to use my own version with regularization.
I now want to try to see how much faster the packed version is.
---
Implementing the packed version has some tricky parts.
I had to find a way to pack the matrix. I will have to undo this at the end.
I had to understand how to use the BLAS functions on the packed matrices. Everything is transpose.
I has to make a local unpacked copy of the diagonal blocks and un-unpack it.
Something is not working still.
I will have to go through each step with a small matrix to see what's happening.
---

*****
12 Mar 2024
The packed version now works.
It is faster than both Lapack and the version that I made of dpotrf. However, the packing and unpacking operations can take a long time.
For now, I will ignore the packed version and use the normal one. 
In the future, I need to understand if the matrices can be produced already packed and if the forward/backsolves can be done in the packed format.
This would bring a decent advantage for large dense matrices.
---
I need to make all these code do the partial factorization.
I tried using a right looking approach in dpotrf, but this slows down the code quite a bit.
I wonder if it is faster to use a left looking approach, and at the end update the Schur complement just one.
---
Yes, using the left looking is quite faster, by a factor 3-3.5 for large matrices.
I tried to compute the number of operations that are required when doing a partial factorization with respect to the full factorization:
if only a fraction p\in[0,1] of the columns are eliminated, out of n, then the number of operations is asymptotic to
(1/3*n^3) * p*(p^2-3p+3)
where the first part (1/3*n^3) is the cost of the full factorization and p*(p^2-3p+3) is the reduction coefficient.
The times that I measure using the left and right looking approach seem to more or less follow this rule.
---

*****
13 Mar 2024
I realized that the left-looking partial factorization can be implemented using two separate arrays for the first k columns and the remaining Schur complement.
This is because the update of the Schur complement is done once at the very end, with a single BLAS call to dsyrk.
One can still use the same array for both, simply providing the correct starting location of the Schur complement block and the correct leading dimension.
I can still use restrict, because there is no aliasing happening between the two arrays.
Doing like this is useful in managing the frontal matrices.
The first k columns will be stored until the end, or copied and immediately destroyed. 
The remaining Schur complement will be kept until the parent is considered and then destroyed.
The two blocks have different lifespan within the code.
Storing them in separate arrays should make it easier to deal with this.
---

*****
14 Mar 2024
I added a function to check that the symbolic factorization is correct.
It creates a dense version of the original matrix and factorizes it using Lapack, with random entries.
It then checks that the nonzeros happen where the symbolic factorization predicts.
With random entries, numerical cancellation is highly unlikely.
If the matrix is too large, the check is not done.
---

*****
18 Mar 2024
I think it's better to avoid overcomplicationg the analyze phase for now.
I will keep the double permutation of the matrix for now.
I will keep the slow row and col counts, without using the skeleton matrix.
If needed, I will come back to these later.
---
Added computation of number of operations required, as the sum of the square of the number of nonzero entries in each column.
Added the print of the summary of the symbolic object.
---

*****
19 Mar 2024
I moved the functions that do the partial factorization into a single file PartialFact.
I also changed them so that they can do both Cholesky and LDL. 
The problem with LDL is that there are no BLAS functions with intermediate diagonal scaling.
To compute A*D*A', for a generic A and a diagonal D: 
- if D is spd, then A*D*A' = (A * D^1/2) * (D^1/2 * A') and dsyrk can be used;
- if D is generic, then I have to make a local copy of A, multiply it with D and use dgemm.
This is slower because it requires intermediate copies, but also because dgemm fills the full matrix, rather than just one of the triangles, like dsyrk.
Indeed, there is almost a factor 2 of difference when using the indefinite code instead of the positive definite.
dgemm may be more optimized than dsyrk, but it still does a more complicated operation.
I need to figure out if there is a faster way of dealing with the indefinite case.
---
To make the copies, I used dcopy.
I read in some forum that this may be less efficient than memcpy sometimes. However, I expect dcopy to be highly optimized, so it shouldn't make any difference if it is properly implemented.
I will stick to dcopy for now.
---
I should also use blocks when building the Schur complement remaining after the partial factorization.
This would use more BLAS calls, but less memory, as only a smaller copy would be needed.
However, this may be considerably slower, similarly to how the right looking approach was.
---

*****
20 Mar 2024
I did a time profile of the positive definite and indefinite dense factorizations, to see where the difference is, for a matrix of size 20000, with 10% of columns eliminated.

     Time profile     Indefinite        Pos-definite          Ratio
        Time copy       0.004962            0.000000            inf
 Time diag update       0.002041            0.002848           0.72
   Time diag fact       0.000515            0.000298           1.73
 Time update cols       0.373991            0.389060           0.96
  Time schur copy       0.043392            0.000000            inf
Time update schur       3.969590            1.965485           2.02

The copies required by the indefinite code do not take too much time.
Updating the diagonal blocks take the same time. Factorizing the diagonal blocks takes twice as much, but this is a very small number anyway.
Updating the block of columns takes the same time.
Most of the time lost is in updating the Schur complement.
I have two ideas to try:
- using blocks when updating the Schur complement
- splitting the copies between positive and negative pivots, scaling the columns with sqrt(|Ajj|) and using dsyrk, i.e.,
  the product A * D * A^T, where D is diagonal with 1 or -1 on the diagonal, can be written as
    \sum_i D_i A_i A_i^T = \sum_{i: Di=1} A_iA_i^T - \sum_{i: Di=-1} A_iA_i^T.
  In this way, I can use 2 calle to dsyrk instead of a call to dgemm.
I think that the factor 2 that arises when updating the Schur complement comes from the fact that dgemm updates the full matrix and not just the triangle.
Using dsyrk should solve this, taking some extra time for the copies.
---
Indeed, splitting into positive and negative and using dsyrk solves the problem. 

     Time profile     Indefinite        Pos-definite          Ratio
        Time copy       0.005025            0.000000            inf
 Time diag update       0.002093            0.002313           0.90
   Time diag fact       0.000510            0.000297           1.72
 Time update cols       0.376947            0.377971           1.00
  Time schur copy       0.035899            0.000000            inf
Time update schur       1.982756            1.980476           1.00

The update of the Schur complement now takes the same time.
The copies can be expensive, if many columns are eliminated (k=n or close).
Hopefully, this is not the case with the frontal matrices, as I expect only a small number of columns to be eliminated.
The update of the diagonal block is cheaper in the indefinite case probably because dgemm is better optimized than dsyrk, and the block is small.
I don't see a point in optimizing more the code for the factorization of the blocks. It takes twice as much, but this time is negligible.
---

*****
21 Mar 2024
I noticed that I also need relative indices to to assemble the original columns into the frontal matrix.
I now have relind_cols for that and relind_clique to assemble the generated elements.
In doing so, I apparently broke something and things were not working properly.
In particular, Metis was giving the usual problems, but worse.
I finally noticed that Metis assumes that the adjacency list of each vertex does not contain the vertex itself...
This means that I should not consider diagonal entries when building the full matrix for Metis.
I fixed this and now everything seems to work properly.
The old segfault and bus error and whatnot were all due to this. What an idiot.
---
I also noticed an error in the way the supernodes are detected. It has to do with Fortran and C indices starting from 1 and 0. 
It should be fixed now.
The issue of the first node not being detected correctly also disappeared.
---
I added a check for the relative indices of the columns.
I don't think this makes much sense, because the only way to check is to almost do the same operations as when I compute them.
I leave it there for now anyway.
---

*****
22 Mar 2024
I tested the analyze phase on the whole Netlib collection.
It works for all problems, both augmented system and normal equations.
I spotted another bug that was giving problems: I was using fsn_parent when it was -1. 
I have to remember to always check for roots when using the elimination tree.
The total times for the analyze phase only are:
normal equations, 0.49s
augmented system, 0.84s
---

*****
28 Mar 2024
I added a double transpose to sort the columns.
It's needed to assemble more efficiently the frontal matrices.
---
I started implementing the factorize phase.
It's a bit of a mess because Analyze takes the upper part and Factorize takes the lower part.
I may need to change the input to analyze.
---
The factorize phase was surprisingly easy to get working.
I'm not sure if everything is exactly correct, I still need to implement a proper check, but for now it seems to work.
---

*****
29 Mar 2024
I changed the frontal matrix from std::vector<double> to double*. 
It gives me a better control over the allocation and deallocation of memory.
There is also no need of any more complicated functionality, so no need for the vector overhead.
Schur contribution of children is now freed after being used.
---
I changed the input to Analyze to be lower triangular, so that both analyze and factorize phases take as input the lower part.
I am not sure if the columns need to be sorted at the beginning of Analyze. They surely need to be when I compute the relative indices using the lower part, but maybe I can skip it for the upper part.
Anyway, for now the columns are sorted with a double transpose.
---
I also changed the input to the constructor to take std::vector, so that I can skip an initial copy.
The code is not meant to be general purpose anyway, so taking the pointer as input does not make much sense.
---
I added a proper Check function.
It computes the factorization using Lapack dpotrf and computes the Frobenius norm of the error, divided by the Frobenius norm of the Lapack factor.
I set a threshold of 1e-12 and is seems to work for all problems.
Interesting that Matlab gives a slightly different (worse) error. Probably it uses Cholmod, that is supernodal, and it sums contributions in a different order.
---
I did some time profiling. qap15 is large enough to see where the time is lost (as usual I initially forgot to switch to -O3...)
Most of the time is lost in assemblying the frontal matrix, rather than factorizing it.
Quite a bit of time is used to initialize Frontal and Clique to zero. Even changing to std::fill or even memset, the time remains high.
Very little time is used to assemble the original matrix A.
Most of the time is used for the children supernodes.
This is 3-4 times the time taken by the dense factorization.
---
It actually is the assembly of the children into Clique that is slow, because Clique is usually larger and has more contributions than Frontal.
It is interesting that the time for dense factorization is smaller (roughly half with qap15) than the time to initialize to zero the frontal matrix.
---

*****
1 Apr 2024
The assemble of children into the frontal matrix now exploits consecutive indices and daxpy.
This reduces considerably the time to assemble the children (by 50% for qap15).
It requires some more memory and some more preliminary computation in Analyze.
---
I improved the time profile of both Analyze and Factorize, and the way in which it is printed.
---

*****
2 Apr 2024
I implemented the merging of supernodes.
I create a new permutation (because children of a supernode may be shuffled), I update fsn_start and fsn_parent.
I need to understand how to merge the columns of the supernode to obtain a representative column.
I also need to perform the symbolic factorization only on supernodes and not on all the columns.
---
I think I managed to generated the sparsity pattern only for the supernode.
I also generated the relative indices and they seem to be right (still have to do a proper check).
---

*****
3 Apr 2024
I adjusted Factorize to use the supernodal pattern.
A little bit of trouble with Check, but I figured it out.
I did some tests on qap15 with different values of max_artificial_nz.
The assembly time becomes considerably smaller. 
The factorize time also becomes smaller, I guess because it does fewer calls to BLAS and each call involves a larger matrix, and this is more efficient.
The prepare also becomes smaller, I guess because even if it needs to allocate and initialize a slightly larger portion of memory, it comes into big continuous chunks rather that small ones.

max_art_nz    sn found    Prepare     Assembly clique     dense factorization
    0           1643       0.50           0.51                    0.37
   16           1399       0.30           0.33                    0.30
   64            759       0.21           0.23                    0.25
  128            581       0.17           0.20                    0.22
  256            421       0.14           0.18                    0.19

Quite impressive results!
It took a lot of effort to implement the relaxed supernodes, but it makes a gigantic difference.
---
Great revelation!!
I don't need to initialize Clique to zero!
I have to initialize Frontal to zero and assemble the contributions there, because they are needed for the partial factorization.
But the contributions to Clique can be summed later, provided that the partial factorization ignores the previous content of Clique (just use beta = 0 in call to dsyrk).
This reduces the prepare time by almost two orders of magnitude, to about 0.005 for qap15.
---
I slightly changed how Symbolic is accessed, to have a less cumbersone code.
---

*****
4 Apr 2024
Changed the way that the Netlib tests are done.
For some reason, the previous way was giving strange errors.
Now the results are printed to an external files, so that it is easy to compare how much time is used for each of the phases.
---
Using -fsanitize=address I found a buffer overflow that I didn't notice. Of course, it was the usual problem the parent being -1.
It also finds some memory leaks, but I cannot figure out where.
One looks to be inside of metis and its size is fixed, it does not depend on the problem.
The others are puzzling, but very small.
---
I plotted the time for each supernode, compared to the supernode size and the frontal matrix size.
There is clearly a correlation with the supernode size. But the times are very clearly correlated with the frontal matrix size. Nothing too surprosing really.
I still have to understand if I should impose a maximum size to the supernodes. This could easily be done by splitting the large ones in FundamentalSupernodes, right after they are found, and then not merging them if they would produce a large supernode.
---
I need to implement the solve phase, which should be easy enough.
Then I need to compare the result with MA86.
I am curios to see how the speed compares.
---

*****
5 Apr 2024
I implemented the solve phase.
Quite easy, no big deal.
---
I included MA86.
Some issues to make it work.
MC68 is giving some strange error, but for now I can ignore it and use the same permutation for both codes.
After linking openmp properly, everything works.
---
I tried on random right-hand sides and the relative error is around 1e-16.
There was an issue with the fact that MA86 accepts only the lower triangle. It does not extract it.
This may have beed the reason Matlab was giving a larger error as well.
No need to check with Matlab anymore now. I use it just for plotting.
The times are weird. For some problems my (serial) code is faster than MA86.
But MA86 uses 8 cores, while mine only 1.
Something is strange. I need to try with MA57, which is multifrontal.
---

*****
8 Apr 2024
I tried different techniques to merge supernodes. 
I would like to find a way that does not depent on a specific parameter that may need to be adjusted for a given matrix.
I tried to merge based on the percentage of fake nonzeros out of the total number of nonzeros in the merged supernode.
I tried also based on the percentage of extra operations required wrt the operations required without relaxation.
I have an idea ot trying whatever strategy iteratively: try once, if extra operations are less than 1% of the total operations, try again with higher threshold.
Continue until you fall within 1% and 2%. If you go over 2%, then reject and try with smaller threshold.
Something like this.
I also think that it is better to choose what to merge based on number of extra operations rather than number of extra nonzeros.
---
It may be useful to merge supernodes if they are too small.
I added a rule to merge a parent with its smallest child, if both are smaller than a threshold.
This is what is used in the HSL codes. However, it does create more fake nonzeros.
I am not sure of whether I should keep the basic strategies (together maybe), or try to find a better one.
---
I understood why MA86 is slower. It does pivoting. I should try with MA87.
Then, MA97 is multifrontal rather than supernodal and I can set it so that it does not do pivoting.
I should include both of these in my comparison.
---

*****
9 Apr 2024
I slightly changed the functions to merge supernodes, so that it is easier to try new strategies.
I managed to get the skeleton matrix for the column counts to work.
It was taking quite a bit of time for large matrices, now it takes a very small amount.
I slightly changed how Symbolic prints the summary.
---

*****
10 Apr 2024
I included ma87 and ma97. I also requested ma57.
They still seem slow. The relative error is small, but maybe I should try with more ill conditioned theta.
---

*****
11 Apr 2024
I included ma57.
It looks quite a bit slower than the others.
The documentation also says that it is not parallel.
---
If I use an amd ordering for the Netlib problems, the time of ma86 is the best, twice faster than my code (run on a single thread).
Also for larger, problems, the hsl codes seem to go better with amd from mc68. Strange, because if Metis 4 was available they would use it to find a nested dissection.
If I use Metis ordering for all of them, my code remains the fastest.
At least until I found the next thing that I am doing wrong...
---

*****
12 Apr 2024
I run some tests forcing all code to use Metis or amd (apart from ma57, which has no option to receive an ordering from the user, but it still find an amd ordering internally).
The results are weird.
First of all, the hsl codes modify the ordering that I provide in some way during analyse.
This modification sometimes seems to break things.
In particular if I provide the Metis ordering, things go very weird.
The number of nonzeros returned in info is also different from the one that I can observe in Matlab if I use the same (modified) ordering.
---
An example with nug08 (all run in serial, pivoting switch off):

             |------------- AMD ------------| |------------ Metis -----------|  
code          factor time       reported nnz|  factor time       reported nnz
ProtoFact         30                1.5e8   |       2                4.5e7
MA86               8                1.0e8   |      20                1.5e8
MA87               6.5              1.0e8   |      19                1.5e8
MA97              16                1.0e8   |      99                1.5e8
MA57              30                  -     |      30                  -

The behaviour seems to be opposite: with metis my code is very fast and hsl is very slow. With amd my code is very slow and hsl is fast.
However, the best time is from my code with Metis. Probably because it has a smaller number of nonzeros.
Indeed, all this strange behaviour could be related only to the number of nonzeros and number of operations performed.
It would be a good thing, because it would mean that my code is on par with hsl (on 1 thread) and the difference is made only by which ordering is used.
Maybe I should install Metis 4 to see if it makes a difference.
---
I am not sure if I understood how Jennifer merges supernodes. If I just merge child-parent when they are too small, there are a huge number of fake nonzeros that are created.
It seems better to go with Ashcraft-Grimes type of merging.
Especially when using the augmented system structure, where there are fewer true supernodes, merging must be done carefully or there are plenty of fake nonzeros added.
---
I slightly changed the second criterion to merge supernodes.
It used to merge the smallest child with the parent, if they were both below a threshold.
Now, among all the children below the threshold, it select the one which produces the smallest number of nonzeros, and merges it. 
---

*****
15 Apr 2024
I found a bug when trying with a larger matrix.
Something to do with the way the indices for the supernodes were counted.
Indeed, I was using a pretty stupid method. Now it's fixed.
---
I found the reason why the ordering seemed so strange.
MC68 returns the inverse permutation and MAxx takes as input the inverse permutation.
What I was providing through Metis was the direct permutation.
Quick fix and now the comparison between the codes, with the two orderings, makes more sense.
---

*****
24 Apr 2024
Back from holiday.
Minor changes to the printing.
Added printing of times for each Blas function.
---
Changed some names of variables to have a more uniform naming convention.
I am aiming at using this convention:
 - CapitalNames for functions and classes
 - thisStyle for class variables
 - ThisStyle for class functions
 - this_style for local variables and other temporary stuff
 - k_this_style for constants
---
Changed constructor of Factorise so that it takes std::vector as input. No idea why it was still using pointers.
---
Small bug when computing the density, n * n was overflowing for large matrix.
---

*****
29 Apr 2024
I changed the solve routines to use BLAS 2.
I moved all the Blas declarations in a single file.
Eventually, the columns may need to be stored in packed format. 
I am not sure how to do the forward and back solve in packed format, since tpsv does not have a 'leading dimension' argument.
Probably the hybrid storage format allows to do something clever about that.
---

*****
30 Apr 2024
I implemented the indefinite factorization as well.
For now, without any regularization, so that it may break down.
I put very large positive and negative entries on the diagonal to avoid it.
There was a small bug, very hard to notice.
When I do the double call to dsyrk to update the Schur complement, I was doing both of them with beta=0.0.
However, the first call was clearing the uninitialized clique. The second call was clearing the result of the first call...
Took a while to notice.
Now everything seems to work.
I implemented the Dsolve and put controls so that things are done properly for normal equations and augmented system.
For augmented system I can only compare with MA86 and MA57. The error is small, as for the normal equations.
---
I implemented the relaxing of supernodes to stop when the number of fake ops is between 1% and 2%, instead of the number of fake nz.
There are small differences, but it looks to be doing better.
I will leave both of them to test later.
---

*****
1 May 2024
I found a better way of using the hybrid packed format.
I can pack the matrix using dcopy and I can leave it packed, doing the solve in a different way.
A couple of things to adapt. I need to figure out how to compute the Schur complement after the partial factorization efficiently in this format.
---

*****
2 May 2024
I checked the old function that uses lower-block-hybrid format to perform the factorization. 
It seems to work.
I added the partial factorization.
I need quite a large buffer to compute the Schur complement in full format, before packing at the end.
There may be a better way of doing this, blocking the Schur complement and using a smaller buffer, but for now it works.
I changed a bit the names of the dense factorization functions.
I will need to figure out how to do the same in the indefinite case.
It shouldn't be too difficult.
---
Next, I need to have a control on whether to use packed or full format in the sparse code.
I need to implement the assembly for packed matrices.
I need to convert to lower-block-hybrid format the frontal matrix and switch to the correct solve.
---

*****
3 May 2024
I started adding the blocked-hybrid factorization to the code, but I noticed that it was quite a bit slower.
It looks like computing the Schur complement using multiple calls to dsyrk is not good.
For the moment, I give up on this and only implement the packed cliques.
The factorization is the same, but the clique is packed after being computed.
This is quite slow, around 20% of the dense factorization time.
---

*****
7 May 2024
I had a talk with Jennifer today.
Some points to note for later:
- frontal matrix and clique should use some kind of packed format. frontal may use full format only for blocks in factorization (nb x nb), rather that for the whole block.
- blas calls may already be using parallelism. htop shows that indeed the cpu usage reaches around 250% (activity monitor is less accurarate apparently). This simplifies dealing with node level parallelism. I still need to deal with tree level parallelism.
- within a supernode, 2x2 pivots may be used without changing the data structure (?). worth considering it when a bad pivot is found.
---
I started to work on implementing the 'layer0' for tree parallelism. It looks like it needs to leave behind a large chunk of operations to obtain balance in the tree.
It may be better, at least for now, to give up trying to avoid imbalance.
---

*****
8 May 2024
To be able to store frontal and clique in packed format, I need to use the hybrid storage format. It is the only one that make sense.
The computation of Schur complement should be done in left-looking approach, block columns by block column. This is described in the doc of MA54 and it makes sense.
It has more blas calls, but if the block size is large enough, it should be able to get good performance. The memory footprint should be reduced quite a bit.
There is the need of a full format copy of the block of columns, but this is much less that a full format copy of the whole schur complement, especially if the front size is large.
---
I am still not 100% sure that the hybrid format factorization is correct.
I need to implement the proper solve routines to properly check.
The time taken is not very different from the packed format.
The copying of data takes some time, but it is necessary.
---

*****
9 May 2024
I checked that the hybrid format produces the correct Schur complement.
I still have to implement the correct solve phase.
It seems to be a little bit slower, due to the extra copies. Not too bad though.
I am not sure if there are savings in terms of memory. A bit harder to check.
---
Changed names to dense factorization functions.
Now the name tells whether the function performs full or partial factorization, posdef of indef, blocked or unblocked, and the packing format.
I also changed some of the error codes of the dense factorization functions.
---

*****
10 May 2024
I implemented the solve for the hybrid format.
It finds the correct solution.
Only for the positive definite case for now, because I am still missing the factorization in hybrid format for the indefinite case.
It shouldn't change much for the solve anyway.
Solve time seems to be a bit smaller when using packed format.
May be a result of using tpsv instead of trsv when the columns are packed.
The difference is quite substantial actually. For nug08 solve time goes from 0.025 to 0.018. 
---
I noticed that indeed the blas calls are more efficient when using the hybrid format.
For nug08, the blas time (withoud dcopy) using hybrid is 0.78, with full format it is 1.03. The dcopy time is roughly equal to the difference, but this may change for larger problems.
It may well become more efficient to use the hybrid format, both for time and memory. Great!
---

*****
14 May 2024
I implemented the factorization in hybrid format for indefinite matrices.
I also implemented the solve with D in hybrid format.
It seemed to work, but if I change the block size, something goes wrong.
I need to step through the code to see where the bug is.
---

*****
15 May 2024
I found the bug.
Very weird one: the makefile should force recompilation of DenseFact.c if DenseFact_declaration.h changes. The block size was in this file, but DenseFact.c was not recompiled when it should have.
So it was using the old block size, while the rest of the code would use the new one.
I moved the block size in another file and put it as input for the functions in DenseFact.c.
I still don't know why it wasn't recompiled.
The depencency file is correct, it contains all the dependencies, so I am not really sure what was happening.
Anyway, now it works.
However, the operation of computing the Schur complement in hybrid format, and in particular the calls to dcopy to put it into lower-packed format, take a very large amount of time.
I need to find a better way. Maybe I can hold the Schur complement packed by rows, instead of columns.
---
I cannot leave the Schur complement packed by rows, because then I cannot use daxpy to assemble it into the parent.
---
Interesting observation of the time taken for dense operations and for copies:
for nug08, in full, packed or hybrid format:

nug08-3rd           Normal equations                      Augmented system
            denseOps    copies    assemClique         denseOps    copies    assemClique
full            1.08      0.00           0.44             3.80      0.00           1.77
packed          1.10      0.23           0.45             3.85      1.54           1.69
hybrid          0.77      0.33           0.41             1.20      1.90           1.39

dense operations are substantially faster when using the hybrid format.
Assembly of the clique also seems to be faster. I don't know why, maybe it is because the clique is stored in packed format, so accessing it uses shorter chunks of data and causes fewer cache misses. Just a guess though.
However, there is a lot of copying needed. 
For augmented system, hybrid format makes sense, because the time for copies plus dense ops is smaller than the time for dense ops with full format.
For normal equations, they are the same.

Another example, from a2864:

a2864               Normal equations                      Augmented system
            denseOps    copies    assemClique         denseOps    copies    assemClique
full            12.8      0.00           3.27             13.4      0.00           9.46
packed          12.4      2.87           2.88             13.1      6.55           7.33
hybrid          10.7      2.18           2.49              4.3      9.20           8.66

Dense operartions are massively faster when using the hybrid format, but copying completely destroys the advantage.
Most of the time spent copying is used for converting the blocks of columns of the Schur complement from full format by rows, to packed format by columns.
I already noticed that I cannot pack by rows, because it does not allow to use daxpy to assemble.
I could pack by rows, keeping the zeros in the diagonal block.
Not the whole upper triangular part, just the zeros in the first block, that stores the diagonal part.
This would increase slightly the memory requirement.
However, there is no more copy needed after computing the Schur completent. I can compute it already in place.
Assemblying becomes more complicated. I need to figure out how to access a specific row/col. 
I should be able to use daxpy though, because with the zeros, the rows are aligned.
---

*****
16 May 2024
I implemented the Schur complement in hybrid format with full diagonal blocks.
It works (after some bugs, I didn't notice that PackType is used also in Numeric...).
However, the assembly is much slower, because the daxpy do not have increment one anymore and they jump around in memory.
I wonder if keeping the Schur complement by columns, but with full diagonal part (not completely packed) is a better option.
The daxpy are efficient, but I still need to copy the Schur complement. Only that now the data should be aligned better and this may yield a more efficient copy. Maybe I should copy by rows rather than columns.
---
There is a little reduction of the time taken to copy the Schur complement, if I hold it in blocks of columns, by columns, with full diagonal part, and if I do the copy by rows rather than by columns.
Not much, for a2864 the Scuhr copy time goes from 9.2 to 7.5, but still it's good.
I wonder if I can assemble the Schur contributions by rows instead of columns. In this way I could leave the Schur complement in hybrid format and assemble in a way that exploits the daxpy with increment one.
---
I have made the hybrid format, with Schur complement packed by columns but with zeros in the diagonal part, the default when using PackType::Hybrid.
Next, I should make PackType::Hybrid2 assemble the Schur complement by rows, rather than columns.
---
There is something absurd going on.
When I run the code on a2864 with augmented system, the code segfaults as soon as it starts factorizing.
No idea why.
If I remove all the changes that I made today, it still happens.
If I git clone the code from yesterday, it runs ok the first time, but if I run it a second time, it segfaults.
Using normal equations is ok though.
Wtf?
I may need to check every single blas call to see if any of them has a buffer overflow. address sanitizer does not do anything, because blas has not been compiled with it...
---
I did some test on some of the Mittelman problems (small ones).
I run both normal equations and augmented system and take the smallest time.
I then run ipx on the same problems and take the average time per iteration.

                      fact          ipx avg
cont1                 0.38            3.40
cont11                0.18           10.42
ex10                  1.23            0.47
fome13                0.27            0.43
irish-e               0.14            0.35
Linf                  0.27            0.89
neos                  0.65            0.69
neos-302              2.70            2.63
neos-505              0.16            0.67
neos3                 1.92            0.80
ns1687                1.38           35.00
ns1688                0.26            0.60
nug08-3rd             1.67            8.00
qap15                 0.27            0.33
rail02                0.47            0.45
rail4284              0.30            2.10
s82                   1.54            6.87
s100                  0.22            0.56
s250                  0.17            0.52
square41              0.02            2.32
stat96v2              0.03            3.01

Clearly, there are problems where ipx has an advantage.
However, some of the problems for which ipx takes a large amount of time can be solved much faster by the direct factorization.
This is very good.
---
I noticed that timing the BLAS calls takes quite a bit of time for larger problems.
I couldn't understand why the percentage did not sum to 100. It's because plenty of time is taken by the timing routine.
I wrapped each call of GetTime() in a #ifdef, so that I can trigger timing on or off.
Quite a bit of saving for some of the larger problems.
---

*****
17 May 2024
A different problem now gives segfault...
The crash report indicates a specific dgemm call that crashes.
It also seems to indicate (?) than it segfaults because it is 1 byte outside of a valid memory segment.
However, it I run the code without -O3, or with -sanitize=address, or if I add some printing to see the indices of memory access, the problem disappears.
I added checks before that specific dgemm call to see if the memory accesses go outside the buffer, but they do not!
Also, sometimes if I recompile or simply re-run the code, the problem goes away and then comes back...
And the problem that was giving segfault yesterday now runs fine.
Wtf
---
I plotted some of the supernodal trees on Matlab, showing the percentage of time taken to process the subtree below each node.
There seems to be a lot of parallelism to exploit, for most of the problems.
Almost identical behaviour if I plot the number of operations per subtree.
I can incorporate the assembly operations at each node by counting them as 100 times more expensive as normal dense opeartions, as a rough estimate.
---

*****
21 May 2024
I implemented the assembly by rows for hybrid2, where the clique is kept in blocked-hybrid format instead of being copied into packed format.
It improves massively the assembly compared to the previous assembly in hybrid2, since the memory accesses are more efficient.
The assembly is still a bit more expensive than the standard hybrid format. However, it avoid any copy of the Schur complement, so it has an advantage against hybrid.
Sometimes it beats Full, but in general it seems to be in between.
It's good, even though the code is getting a bit complicated.
I will keep both for now, to see which one performs better when I do some proper testing.
---
I made some changes to the function to generate the starting layer for tree parallelism.
It now prints the expected speedup using a certain number of processors.
The total number of operations used is equal to the number of dense operations, plus the number of assemble operations multiplied by 100.
This is because, on my computer, I observed that assembly ops are roughly 2 orders of magnitude slower than dense ops.
This may be different on other computers though.
The speedup depends a lot on the specific problem and on the structure of the matrix.
Some problems have a very large speedup, close to the number of processors used.
Some problems have almost no speedup possible.
Better performance will be obtained using node parallelism as well.
---

*****
28 May 2024
I switched the BLAS calls to use the underscore. Native BLAS makes no difference, but to use openblas it only works with underscore.
I tried openblas and indeed it reached 450-500% cpu usage. Native blas reach 100%.
openblas is much slower, even if it is parallel. 
Using native should always be preferred, unless there is a reason not to.
It looks like syrk in openblas is worse than gemm. I may need to switch all syrk to gemm, just in case. gemm is usually the most optimized function.
I need to implement node parallelism by hand, because BLAS may or may not be parallelized. Also, it is more easily controllable.
---

*****
30 May 2024
Openblas actually works better when run in serial. Maybe the problems are too small, or the block size. I tried changing it, but it does not seem to improve much.
It does not seem to give segfault though. But I need more tests.
I also tried slightly increasing the size of the buffers (by a few bytes), using native blas, and I don't get segfault. But again I need more tests.
---

*****
18 Jun 2024
Segfault in GenerateLayer0 fixed.
I had to check that the node to be removed is not a leaf and in that case break.
---
I changed GenerateLayer0 so that layer0 is order reversed. Now the node to remove is always the last one and can be removed easily.
---
I implemented the reordering of the children to optimize the stack size.
Something does not work though.
I find the correct supernodal permutation, but then I need to update the data structures, and something goes wrong.
---
Problem fixed. I wasn't permuting all the vectors that are needed later.
I also added an estimation of the maximum amount of memory needed for the frontal matrices.
---

*****
30 Aug 2024
Many things not logged during summer.
Mainly, changes to protoIPM to accomodate more easily the linear solvers, refactoring of the naming convention, to align with HiGHS convention.
I am learning cpp multithreading, but it takes time... 
I will need to meddle with Leona's parallel interface, to make sure that I can do what I need to do.
---
I still need to plug FactorHiGHS into ProtoIPM to see what happens. I will probably need to add regularization.
---

*****
9 Sep 2024
I added FactorHiGHS to ProtoIPM. It solves some problems, but for most of them it stops due to invalid pivots.
As expected...
I need to understand how to add regularization to make it work.
At the moment the code runs with two solvers, to compare the solution to a solver that I know works.
So many strange bugs trying to fit together many different solvers. I don't want to fix them, because they will not be used for long. MA57 still does not compute the correct things, but I will give up. I have 86/87/97 that work.
---

*****
12 Sep 2024
There is a bug with the augmented system solver, it does not find the correct solution.
This only happens when I put reasonable pivots, instead of the large ones that I was using during development.
Luckily, the problem appear already with afiro, which is very small and easy to handle.
On Matlab, I can compute the ldl and see that there are no 2x2 pivots, so the problem is not related to pivoting.
---
The problem only happens with hybrid and hybrid2 formats. 
With full format, everything works well.
I need to figure out what is going wrong, probably something to do with the dense factorization.
I can see the wrong entries when comparing the matlab factor to my factor.
---
Found the bug. It was in the dense factorization code for the two hybrid formats.
When I had to scale the columns by the pivots, after the factorization of the diagonal block, I was reading the pivots from the original storage A, but the pivots had been updated in the temporary full storage copy D. So I had to read them from there instead.
This was not creating problems before, I think, because the pivots were all very large and similar in magnitude. So they weren't affected much by the rest of the operations.
Using smaller diagonal elements means that the magnitude of the pivots is much more varied.
Very challenging bug to find...
---
Now everything seems to run as expected in ProtoIPM, in the sense that the code stops for invalid pivots, and the early directions found are accurate (around 1e-12 or better comprared to other solver).
---

*****
13 Sep 2024
Bug was not completely fixed. I forgot to initialize the starting position to zero, since I am now accessing D and not A.
It was causing issues for larger problems.
---
I changed the names of the hybrid formats and consequently also the names of the dense factorisation routines.
It makes a bit more sense now.
I introduced a format handler, to separate the implmentation of the various operations for various formats.
Now there is an abstract class that handles a generic format and various implementations of it, depending on the format.
---

*****
17 Sep 2024
I added the pivot signs for the indefinite factorization.
They are held inside Symbolic and passed around the dense factorization routines.
At the beginning, I used a std::vector<bool> to save some space, but I realized that when I pass a bool* to C, coming from a std::vector<bool> in C++, I don't think that I can access the correct information.
C will assume that bools are stored one in each byte, when actually they are stored one in each bit, since they come from a std::vector<bool>.
So I use a std::vector<int>, which wastes a bit of space, but it shold be negligible compared to the rest.
---
I just lost two hours to debug something. I was getting super weird errors, it seemed like the symbolic factorization was wrong. But it was working on the first iterations.
Was something modifying the symbolic object? some buffer overflow? you'd think right???
No, it was that the function that Julian wrote to compute the normal equations removes small entries...
I knew that, I just forgot that it would have an effect at later stages of IPM, because some entries do become very small.
Since I am reusing the symbolic factorization, they did not match anymore.
---
I am now getting convergence with normal equations for many problems.
I do not get very big negative pivots anymore (so far).
I do get pivots which are barely negative, easy to deal with regularization.
This looks good now.
---
I added some simple regularization, it solves some problems but it fails on some others.
The problems seems to be that if I regularize with a small value, then the rest of the matrix is factorized completely wrong and very large negative pivots appear.
---

*****
18 Sep 2024
I added regularization that depends on the maximum diagonal element of the matrix.
I scale it down by machine precision, as Jacek suggested.
It solves the problem that I was having with d6cube, but it not enough for qap12.
The troubles appear to be all in the same supernode and in the same block (last and last).
I should try to implement the look-ahead heuristic of Jacek on a given block, it may be enough.
---
It's actually quite easy to implement:
p is the pivot (after sqrt)
b is the column below the pivot
d is the diagonal of the block

The update would produce d_k <- d_k - b_k^2 / p^2, for each k
We want d_k - b_k^2 / p^2 \ge tau^2, for some threshold tau
Therefore, p^2 \ge b_k^2 / (d_k - tau^2), for each k
It seems to work, but I don't get convergence for qap15. The factorization does not break down, so I suspect that I need some refinement now.
Indeed, comparing with a second linear solver, the error shoots up as soon as regularizaiton kicks in.
---
I actually got it wrong. Jacek told me that he avoid sqrt, so all the check are done to the pivot, before square root. 
My numbers for the factorization threshold are wrong then.
I still want to keep both factorizations available. I may switch everything to ldl if it works better, but for now I want both to work.
He also explained to me that he does not only check the pivot agains the column, to detect when it will create problems, but also against the row.
If the pivot is much smaller than the row, then this is row is basically linearly dependent on the previous ones, and can be ignored.
This is done by putting a large (infinite) pivot, to ignore this row and column.
To do this, I need to keep track of the largest element seen so far in each row.
Reading a paper about this, it also seems smart to store the inverse of the elements of D, rather that the elements themselves. 
In this way, I don't need to do divisions (which are expensive) and I can just set the inverse entry to zero if I want to ignore a row.
---

*****
19 Sep 2024
I changed the option of protoIPM, there is no more option to choose predcor, but there is to choose the format.
---
I realized that I cannot store the reciprocal of the pivots, because I need to multiply columns by the pivots in multiple parts of the code (for ldl).
For Cholesky this is also not possible, because I use trsm with diagonal.
I may change all the pivots to be the inverse when the factorization is done, so that the infinity pivots can be zero, but for now I leave it as it is.
---
I changed the regularizaiton so that it applies before the sqrt.
The problem now is that I regularize too much. I need the refinement.
Sometimes however I seem to regularize too little.
Strange behaviour.
---
I realized that I don't need to pass around a pointer to the whole pivot_sign vector, I need to pass a pointer to the portion that corresponds to the supernode or block.
The signs should be correct now.
I added the regularization to the augmented system, taking into account the sign that the pivot should have.
---
I changed the option of ProtoIPM again, so that now I can select what approach to use (aug sys or norm eq) and also which factorization to use (chol or ldl).
Factorizing the normal equations with LDL works, and actually seems to be more precise. Jacek mentioned that this may happen, because the sqrt could be problematic (??).
---

*****
23 Sep 2024
I changed the timing, so that it propagates throughout the IPM iterations.
It is now stored in Symbolic and passed around the various routines.
I added some #define, to have three levels of timing.
---
I changed DenseFact, so that there are no #ifdef TIMING_3 around anymore, since it was making everything complicated to understand.
Now, the BLAS calls are done through an interface called callAndTime_xxxx, so that if timing is required, the times are automatically recorded.
---
I added a record of the regularization that has been added.
I need to undo the permutation to apply properly when computing the residuals during the refinement.
And then I need to fix the refinement.
---
The calls to dense_fact are becoming too complicated, with too many arguments, and I still have to add more things.
Maybe I should turn them into non-virtual member functions of FormatHandler. 
The object could hold the data, that then doesn't need to be passed around as argument.
I thought also that I could make the whole Factorise abstract, without relying on FormatHandler. But I want Factorise to be part of the external interface, so it's better to avoid it.
---

*****
24 Sep 2024
I decided against moving DenseFact to cpp, inside FormatHandler. For now at least.
---
I realized that both approaches are already using a default primal regularization, because it is incorporated into scaling.
I added the default dual regularization also to the normal equations, and it seems to fix dfl001. 
---
I fixed the iterative refinement.
It now considers the regularization, both static and dynamic.
Now, the refinement is triggered only if the relative residual is above 1e-8.
It is almost never triggered.
For cycle, it triggers a lot: in the early iterations, it reduces the residual. In some of the late iterations it makes it worse. Weird.
I now reject the correction if the residual grows.
---
I managed to get greenbea to converge, reducing the static regularization to 1e-12/1e-10, instead of 1e-10/1e-8.
pilot.we still struggles though.
---
Mittelman struggles a lot.
It looks as though it does not get directions that are accurate enough.
The IR fails many times. There must be something wrong.
---

*****
26 Sep 2024
I added the scaling of matrix A, taken from IPX.
I need to figure out if there is any other scaling to apply. 
I remember Jacek mentioning that he would scale the costs or rhs, or something else.
---
I fixed the segfault during setup.
It was caused by computeAAT failing, because the matrix has too many entries.
It risks overflowing the pointers.
If that happens, I changed setup to return an error and the ipm stops.
I also added a check for the number of bad iterations inside the ipm, i.e., the iterations with small stepsize. If it happens too many times, the code stops.
---
I tried to run the ill-fated dgemm instruction with my own implementation, using address sanitizer, but there is no buffer overflow. I really don't understand what is happening.
Printing some data, it looks as if the segfault always happens when I access the last entry of A, and the crash report says that I am off by one. Weird.
For now, I fixed (I hope) by allocatin a few enrties more to A.
It seems not to segfault anymore.
---

*****
30 Sep 2024
I removed perm_ from Symbolic. All permutation can be done with iperm_, storing one fewer vector.
I also modified computeAAt so that it computed the lower triangle. I still need to keep the original function for the other solvers, but it can be removed once the other solvers are not used anymore.
I removed the assert from getPermutation and added a return value.
I thought of using BLAS for VectorOperations, but all the componentwise operations do not have a corresponding function. 
There would be little advantage then. I will leave them as they are.
---
I added matrix equilibration inside Factorise, so that the matrix is scaled before being factorised.
However, this does not seem to help. Actually, it hurts convergence in some cases.
I think that some entries are pushed to be too small and this triggers too much regularization.
It may need to be tuned better.
---
I switched the scaling to the standard one, instead of the one that Lukas is using.
It seems to behave better. It attempts to produce a matrix with rows and columns of /infty-norm equal to one.
Some problems are better with scaling, some problems are better without. However, I think that having the scaling is better because it creates univorm absolute values that I can use to regularize.
---

*****
01 Oct 2024
I changed the printing, so that I can choose whether to print the standard data or the extended data.
Extended data include max and min regularized Theta, max and min entry in D (pivots), max and min entry in L, number of regularized pivots, max regularization applied, max residual after refinement.
---

*****
02 Oct 2024
I found a source of inaccuracy in the dense factorisation.
When I update the pivots, I subtract the result of ddot from the current pivot.
If I subtract the single contributions from the pivot instead, there is less loss of accuracy (kind of like modified Gram Schmidt?).
This removes the problem of pivots going to exactly zero, even though it seems like it still happens sometimes.
It looks to make things better, because if the pivot goes to exactly zero, it means that all information has been lost and I can only regularize with a random value.
---
I changed the code so that instead of computing the corrector, I directly compute the final direction. Hopefully, this helps in reducing inaccuracies in the direction.
---
I cannot understand if the refinement is correct or not.
The residual sometimes are large, but I cannot find a reason.
---

*****
03 Oct 2024
I switched the summation of the pivots to be the Kahan compensated summation. This should further reduce the pivots that suffer from numerical inaccuracies.
---
I set a combination of regularization that seems reasonable. It still struggles with greenbea/dfl001.
I think that Jacek in hopdm does something different than what I am doing here. I need to check better.
---

*****
07 Oct 2024
I switched the scaling of the LP to Curtis-Reid, considering the matrix A and also b and c.
It seems to work fine.
I should now switch to CR scaling also for the linear system, to see if it makes a difference.
---

*****
08 Oct 2024
I switched the dynamic regularization of the pivot to be a separate function, so that both cases of upper and lower triangle can call the same.
I think that I should change the static regularization, so that it is added only after the pivot is computed. 
If a pivot is large and I add 1e-12, that will be completely lost, but it may still be relevant when things are subtracted later.
---
It seems like matrix equilibration works better than Curtis-Reid for the linear system.
I changed equilibrate() so that it uses powers of 2.
---

*****
09 Oct 2024
I refactored Ipm quite a bit, to make it a bit better looking.
Of course, it took two tries because I didn't check if it was working in the meantime enough times and I couldn't fine the issue, so I had to go back...
---

***
10 Oct 2024
I managed to produce an allocator for std::vector that does not initialize the elements. This would be good for clique. 
However, I tried to use that, but the factorization gets slower.
It's weird, because it's only the dense factorization that is affected, but that should only work with the underlying pointer. It should not care where it comes from.
---
I think I understand the problem.
The issue is not the initialization. 
The problem is that if the vector is initialized at the beginning, it is brought into the memory straight away.
If instead I allocate memory and not initialize, the vector is only mapped, and it is actually copied into memory at the first time that something modifies it (i.e. the computation of the Schur complement inside denseFact).
Indeed, the two approaches take similar time only that the time is spent in different point of the codes.
I should then simply use a std::vector, without worrying.
I may decide to implement a specialized allocation tool for the cliques, so that I don't have to allocate and deallocate vectors, which would reduce the problem.
However, this represents a small fraction of the total time (apart form full format, which has big blocks to allocate).
---
I also changed the order of operations, so that assembly into frontal and into clique both happen before the partial factorization.
This makes the code simpler and does not require complicated stuff to overwrite the Schur complement during the dense factorization.
It does requires using axpy instead of copy when putting stuff into the Schur complement using HybridPacked format.
---
This also makes me think that doing the copy of the Schur complement with HybridPacked format is not that big of a deal. The time was actually spent bringing the data into memory.
And some of the advantage that I was observing for BLAS using the hybrid format, was actually given by the fact that instead of spending time to bring data into memory during syrk, I was spending it during copy.
I need to do some tests to see what the real advantages are.
---
It seems now obvious that if I allocate large amount of memory, for the clique for example, this is almost surely mmapped by the OS, rather than being given by malloc.
And the larger the chunk of memory, the more likely it becomes. And supernodes tend to become larger and larger.
Of course, if I receive a brand new page of memory, it has to be initialized, or I would read data from another process.
What I missed is that the initialization happens even if I don't do it explicitly. It's done the first time I actually try to write to the chunk of memory.
A custom allocator for cliques seem the only possible way of avoiding the constant zeroing of the newly allocated memory.
---

*****
11 Oct 2024
I changed the computation of memory requirement in Analyse, so that it is done considering the format used.
This required changing the way the format is set in the Symbolic object.
This will help compute the maximum stack size for cliques, when I code the allocator.
---
The static regularization is now added after the pivot is computed. This should avoid some of the issues.
---
I added the calculation of the maximum stack size. This is quite similar to the reordering of children to minimize total storage.
---
I changed the way the informatin about layer0 for parallelization is stored and printed.
---

*****
14 Oct 2024
I implemented the stack of cliques. I made a separate object CliqueStack, that is persistent throughout the calls to Factorise, so that the memory does not get reallocated each time and does not need to be paged in multiple times.
The code is slightly faster now, but not that much. There is a little gain in the prepare time. At least, the times for the BLAS calls are not "corrupted" by the paging in of the required memory.
---

*****
18 Oct 2024
I spent the last few days trying to understand how to use the parallel scheduler in HiGHS. While doing that, I realized that it may be better to spawn separate tasks for each supernode, rather than to split the tree in subtrees. The scheduler of HiGHS allows this easily, and it would also easily incorporate the node parallelism later on.
Essentially, I would spawn the task of the root nodes. Then, each node would spawn its children. It would perform any preliminary work and then sync with one children at a time (in a fixed order) and assemble the frontal matrix. Then peform the dense factorization.
The use of work stealing and other complicated scheduler stuff should make this more efficient, in particular in keeping a good balance between processor. 
Each node writes to a different memory location, so it does not require locks. 
There is the issue of false sharing. I am thinking to align each chunk of columns and each Schur complement to the cache lines. 
Alternatively, I can have a thread local copy of them, write to the local copy, and only at the end std::move the result in its final position. No one should access that until the child has finished anyway.
This approach does not allow to use a stack for the schur complements, so I have to revert back to the previous technique of using a separate std::vector for each Schur complement. Not a big deal.
I also don't need the layer 0 anymore.
I need to be careful to spawn the children in reverse order, because spawn tasks are put in a stack, so the first to sync is the last to have spawned. I wrote a function to invert the linked lists of children for this.
---
I changed FormatHandler so that it holds a local copy of frontal and clique. It performs all operations on the local copy and std::moves them in their final position when detach() is called.
This actually simplifies the whole thing. I should have thought about it before.
---
Another issue is that times should not be measures within Symbolic. 
I should create a separate object to collect times and other data.
Then, each thread could have its own object, and the times can be added together at the end.
---

*****
22 Oct 2024
I moved the statistics about the factor from Symbolic to DataCollector.
Symbolic is now cleaner and only contains information used later by Factorise. 
---
I changed the FormatHandler to be a local variable inside processSupernode.
In this way, each node has its own and different threads don't access the same.
This required a factory of FormatHandlers that returns a unique_ptr.
It also required FormatHandler to have a virtual destructor (for some reason).
I also modified FormatHandler so that the function init is now performed in the constructor.
I was hoping to include terminate in the destructor, but it needs to know where to move the local copies. I could do this with extra parameters in the constructor, but it's annoying. I could do it with a custom deleter, but it becomes overly complicated with little benefit. I leave it as it is for now.
---
Now each supernode has its own local regularization, that is summed to the total one at the end.
---
I added a flag to stop the computation. If something goes wrong in a supernode, the flag is set. During the computation, other supernodes should check the flag and stop if it is set.
I think the flag should be atomic, but it probably works with a normal bool (?).
---
Adding a lock to the record of times requires to access it in a specific way.
I cannot do it now, because DenseFact accesses the whole vector, since it is in C.
I need to change DenseFact to cpp before doing this.
I need to do it anyway, since DenseFact should eventually use the scheduler as well.
---

*****
23 Oct 2024
I switched DenseFact to cpp. Now there are no c files to compile.
I left the functions C-like, instead of making them into an object. It doesn't seem worth it to over complicate things.
I have a small issue with the timing. The time for fact and convert now include the time for the BLAS calls. I don't want to create a fourth level of timing though. I will leave like this for now.
---
I have changed the DataCollector so that the times and regularized pivots are set with a function.
This will allow the use of a lock when the code is parallel.
The lock will need to be used also for extremeEntries.
I also modified the way the maximum regularization is computed.
---
I put together DenseFact HP and HH, since the code is almost identical.
I have an extra parameter format, that is used when computing the Schur complement, so that it is computed in the correct format.
The rest of the code is identical.
I should implement another format, similar to Full, but that only stores the blocks and not the full matrix.
It should be quite similar to the hybrid code, but the BLAS calls change.
---

*****
25 Oct 2024
Yesterday and today I worked on implementing the new format.
Weirdly enough, it worked immediately. I just need to understand if it does what I think it does.
It should be correct anyway.
At the moment, clique_block_start is computed once and stored in Symbolic, while the equivalent for frontal is computed only when needed.
I will need to decide whether to compute both before or both when needed. Not a big deal anyway.
Numeric has become a bit of a mess, I may decide to have a separate object to handle the various formats there as well. Or not, we'll see.
---
There was something wrong, I was allocating way more space for the frontal matrices.
Easy fix, I just considered only the case of square matrix, but I have to consider that the frontal matrices are not square.
The memory estimate is now much smaller than full format and slightly larger than the hybrid formats.
---

*****
29 Oct 2024
I made some changes to the printing of the regularization of the pivots, so that I can identify which pivot it is.
It does look like the issues travel up through the elimination tree, as expected.
I noticed that if I increase the static and dynamic regularization, there are fewer issues, but the convergence of IPM may be affected.
Still, I think the best approach is to keep the regularization a bit high, so that the factorization does not fail, and deal with algorithmic issues in some other way.
---

*****
30 Oct 2024
I added the locks to DataCollector and changed a bit the interface, so that it should be safe to access concurrently.
---

*****
31 Oct 2024
I added the spawn and sync instructions to use the scheduler.
I made a mistake at the beginning. The lambda that I use to define the task should take the supernode number as a copy, not a reference, obviously.
It finds the correct solution and takes a bit less time.
I added the calculation of the critical path along the elimination tree, to estimate the maximum speed up possible.
For some problems, there is a noticeable difference. fit2d has a 2x speedup.
There may be some contention of the locks of DataCollector. I should try to use atomic for some things.
---
I wonder if it makes sense to reoder the children rather than based on memory (since I am not using the stack approach), based on the amount of work needed for them.
Would it make sense to sync first with large or small fronts? Not sure yet.
It may also depend on which supernodes are on the critical path.
---

*****
1 Nov 2024
I checked the critical path for some problems and it seems correct.
It is not surprising that for many problems the critical path takes a large portion of the total operations.
Most of these are in 1 or 2 very large supernodes.
Proper parallelism will only be achieved with node parallelism.
---
I made some slight changes to the timing.
I also confirmed that the contention of the lock of the timer in DataCollector is significant, especially with TMING_3 enabled.
This is not an issue, since timing is only used for development and will disappear almost completely later on.
Also measuring some statistics, like max reg or number of reg pivots, uses locks. I can use an atomic operation for the number of reg pivots, but I don't know if it's worth it for the max regularization.
Anyway, this should not make much of a difference.
---

*****
5 Nov 2024
After speaking with Jennifer, I realized that I was doing the parallelization slightly wrong.
I should wait until the first child is finished before allocating the parent, otherwise there is extra memory required.
I changed the code and the performance seems unaffected.
---
I should also try to spawn the leaves, and spawn the parent as soon as the first child is finished.
However, doing so means that I don't have an easy way to check when the whole tree is finished.
If I spawn the roots, when they finish the whole thing finished.
If I spawn the leaves, I need some way of knowing when the whole tree finished. 
I am not sure how to do that. If I use a flag, I should constantly be checking on the main thread, which means that the main thread would not be doing much work.
For now I will leave the spawning of the roots. Jennifer seems to think that the two ways of doing it are exactly equivalent.
---

*****
6 Nov 2024
I think that I need to implement delayed pivots within a certain block.
This would help a lot for stability. It does require rows/columns swap within a certain supernode, but some of this may be done in parallel.
It also requires storing a local permutation of the indices of the columns of the supernode, to be used during the solve phase. I still need to figure out the details.
---

*****
7 Nov 2024
I'm making progress in understanding how to implement rows and cols swaps.
I also refined the timing, so that it shows the time for the main columns and for the Schur complement separately.
---

*****
11 Nov 2024
I have implemented node parallelism, parallelizing the calls to gemm and trsm, when computing the main part of the factor and the Schur complement.
It does not seem to bring a large benefit though. Maybe there is too much contention on the cache lines when writing the result? Can I do anything about that? Maybe store the blocks with some cache alignment.
To use the scheduler, I could not pass directly a lambda that calls CallAndTime_xxxx, because the task object wat too large (not sure what is happening, I guess too many parameters to capture).
So I created a separate object only to call gemm and trsm. The lambda only captures the pointer to the object and it works.
It may be possible to play a bit with when the tasks are synced, but this depends on which pivoting strategy we go for.
---
I checked the times for nug08 on the Mittelman benchmark. Last time I checked, my mac was taking a comparable time to solve. ipx takes 166s on Mittelman, it takes 18s with my code. Good progress.
For copt, it takes 0.3s, but it does a very aggressive presolve and folding. My code does ~50 times more flops and takes ~60 times longer. We are not that far in terms of performance of the linear solver.
It may be worth starting to think if we can improve the presolve or implement folding.
---

*****
18 Nov 2024
After talking with people at RAL, and reading some papers they suggested, I realized I have to make some changes to the dense factorization, to make it slightly better.
I start with changing the kernel to be right looking.
This is in preparation to change the whole order for the updates of the blocks, based on the Kauffman paper about LAPACK.
In this way, I don't need to make a copy of the row multiplied by pivots, I simply save the column before scaling down and do all the products. Slightly more efficient memory accesses, but I use axpy instead of gemv.
I tried to use dger, but for some reason it gives segfault.
There is a slight time increase, negligible compared to the rest. I may decide to go back on this in the future though.
I also don't need the compensated sum anymore, because the pivots are computed in a different way.
---


*****
To do:
- row growth factor (and store inverse of D?)
- check iterative refinement

- free variables
- limit growth of x and s?

- Bunch-Kauffman within a block (Schenk-Gardner paper)
- other variations from Ashcraft-Grimes-Lewis, for dense or sparse
- change order of updates, from Kaufman paper