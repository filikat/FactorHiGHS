*****
26 Feb 2024
I start implementing the analyze phase.
Most of the analyze phase can be taken from Tim Davis' book.
---
I need to choose which format of the matrix to accept as input, full or upper triangular.
I decide to accept both. 
If input is full, I extract the upper part.
If input is upper, nothing else is needed.
---
Function to permute the matrix symmetrically into upper triangular form is taken straight from Tim Davis.
I use this function also to extract the upper part from the full matrix, simply by using the identical permutation.
---
Function to get a permutation from Metis needs the full matrix, because Metis does not take triangular matrices for symmetric graphs.
If the original input was upper, I need to create a temporary copy of the full matrix. This is similar to Permute.
If the original input is full, I can save a pointer to it to use for Metis. Only that Metis takes non-const pointers.
I need to make a local copy of the full matrix to pass to Metis. Annoying, but not a big deal.
---
Function for the elimination tree is straightforward. 
The tree is saved in the vector parent, where if the parent is -1, then the node is the root.
The function in Tim Davis book works only for upper triangular matrices. I initially go with lower triangular, but I have to change the format to avoid over complicating things.
---
I need to implement the postorder, which should also be straightforward. 
But I am not sure if I should permute the matrix again. Maybe I can avoid it somehow.
In the future, the analyze phase should include also the ordering to reduce stack size for frontal matrices. For now, I ignore this.
---
I have two matlab functions to check that everything works on some small example. 
They convert between Matlab matrices and CSC matrices. 
After some debugging, everything runs smoothly.
---

*****
27 Feb 2024
I implemented the post ordering. 
Pretty straightforward, taken directly from Tim Davis.
At the moment, after computing the postorder, the elimination tree is relabeled and the matrix is permuted.
Also the vectors perm and iperm are updated, to keep track of the overall permutation applied to the original matrix.
I added some auxiliary functions to manage the permutations and moved them into a file of auxiliary functions.
---
I had some issued when trying with larger problems. I spotted some issues with the Matlab functions that I was using to generate the CSC matrix.
I fixed it and now it works for other problems as well.
I made the Matlab functions a bit more automatic, so that everything runs more easily and faster.
---
I am still not sure of what's the best input form.
As Julian pointed out, taking a full matrix is dangerous, so I may go back to taking only upper triangular.
Generating a local copy of the full matrix is not a big deal.
---

*****
28 Feb 2024
I spent a lot of time trying to use the algorithms in Tim Davis book for row and column counts. They don't work for some reason.
I will go back to that later. For now I am using the slightly inefficient approach.
It uses O(|L|) operations instead of O(|A|). Not a big deal, but it could be improved. 
For now everything works. I implemented the row and column counts and the full symbolic factorization.
Still missing supernode stuff.
I also need to decide how and what to store at the end. I may need a struct SymbolicFact to store the permutation, elimination tree and sparsity structure of L.
I am still trying to understand if I can avoid a second permutation with the postorder of the matrix. 
It should be possible, but I need to figure out some details. Not a big deal anyway. 
---
Some matrices, like pilot (but also others when using augmented system) give issues. 
There is a segfault during the call to Metis that I cannot understand.
I need to try and build the matrix directly in cpp and see if it persists. Maybe I am doing something wrong with the way I create the matrix in Matlab.
---

*****
29 Feb 2024
I implemented the fundamental supernodes. The algorithm to find them comes directly from Scott&Tuma book. 
For some matrices the supernodes are quite small, especially when using augmented system. It may be needed to merge them. For now I leave as it is.
Supernode information is stored as starting node of each supernode (fsn_ptr) and supernodal elimination tree.
An intermediate vector of length n is needed to find the tree efficiently.
---
I created a separate object Symbolic to store the information of the analyze phase. 
The idea is that an Analyze object creates a Symbollic object, and this is never modified again.
I need to adjust a couple of things because at the moment it is a bit of a mess.
No need to std::move everything if S is passed at the beginning and things are stored there directly.
Everything that is not needed after the analyze phase is discarded.
Symbolic is then accessed only by const public functions.
---
I implemented the relative indices for the frontal matrices, and also the extraction of the information of the clique tree.
There is an issue if the supernodes are merged, because the information about which rows are nonzeros in the frontal matrix cannot be read directly from L anymore.
The columns of the supernodes that were merged have to be merged as well and stored separately. 
I will think about this if and when we decide to merge supernodes.
---

*****
1 Mar 2024
Removed option to input full matrix.
Now, any matrix is given, it uses permute to extract the upper triangular part.
I decided to keep Analyze and Symbolic as they are, with std::move, instead of passing S at the beginning.
---
I changed test.cpp to directly read the mps file of the problem and build either the augmented system or the normal equations.
It can also presolve the problem. 
The Matlab test file now reads everything as input from file, instead of building the matrix separately.
---
For some problems (e.g.,pilot and woodw), Metis gives some weird errors, but only sometimes. The errors are not even always the same.
It gives segfault, bus error, trace trap. Backtracing in LLDB shows that this happens during the call to Metis.
The problem becomes less frequent reading the matrix directly from mps file and presolving (for pilot at least), but it persists for woodw.
No idea what is happening.
---
Also, the reordering using Metis is slightly worse than the reordering using dissect in Matlab.
It should use Metis underneath, but I cannot reproduce the same results by changing the options.
Maybe this only happens for small matrices, because amd is used at some point?
---
I improved the comments in Symbolic.h, so that everything is explained properly. 
I added a small example to show what the vectors contain.
---

*****
4 Mar 2024
I implemented the dsyrk BLAS function.
I basically copied it from the Netlib Fortran code and translated it to C.
This is the most basic implementation, not optimized much. 
It does access everything by columns, to get better data locality, but there's no other optimization.
Indeed, comparing it to the BLAS function already installed on the computer, there is a factor approximately 50 of difference.
There is basically no hope to compete with BLAS.
I think that some kind of divide-and-conquer may help, but we will never match or get close to the optimized BLAS.
I think we need to have the option to use pre-installed BLAS and Lapack.
---

*****
5 Mar 2024
I implemented dtrsm.
I improved the tests for the BLAS functions.
I can see that the time to solve the different subcases (there are 16 for dtrsm) changes considerably for different settings.
The transpose option seems to have a big effect. This is probably due to how the memory is accessed. 
However, trying multiple combinations and different sizes, the behaviour is still not too clear.
An important thing is that, to do the operations C = A*B, it is more efficient to store C and B by columns and A by rows.
If A fits in L1 cache, this is sometimes called data streaming, because the columns of C and B are streamed into cache one by one, while A is always present.
So, for example, doing A^T*A should be faster than doing A*A^T, if A is stored by columns.
I observe this, but if I reverse the size, the behaviour changes. There are probably multiple factors acting.
---

*****
6 Mar 2024
I moved the BLAS functions from cpp to c.
Now I can use restrict on the pointers of the matrices.
This does not bring any visible benenit though. Still, it's better to keep it.
---
I implemented dgemm.
I tried using loop unrolling on the functions, but it does not change the overall time.
I had a look at the disassembled object code, and simd instruction are already automatically used with -O3.
Cache optimization must be responsible for a large part of the performance difference compared to native BLAS.
I should try blocking the code, as in dpotrf2.
I finally understood how that works btw. I understood wrong the use of leading dimensions (lda,ldb,ldc) in BLAS.
I updated the access to the arrays to use lda and blocking the code should be straightoforward.
I wonder if using indexes like dpotrf2 is actually the best thing for cache efficiency though, especially if the columns of the matrix are not aligned with the cache lines.
Copying the matrix in a smaller array would surely make the data more compact and thus improve cache efficiency, even though one has to copy everything.
I should try both approaches.
---
I tried splitting the matrix as 2x2 blocks of the same size. 
I then applied recursion until a base case is reached (where sizes are smaller than 32).
For dgemm, this bring a benefit for some cases (transa is 'T'), but makes it worse for others (transa is 'N').
---
Another important thing to notice is that, to achieve good cache efficiency, the columns of the matrix should be aligned to the cache lines.
This does not happen unless the size of the marix is a multiple of the cache line size (128B for my mac, or 16 double).
It may be useful to copy the data into local storage and add padding until the next multiple of 128B.
I am not sure how much this affects the performance, but I should try the approach with local copies.
---

*****
7-8-9 Mar 2024
I went into the rabbit hole of understanding how the optimized BLAS are implemented. 
It's very complicated and only makes sense if one writes assembly code.
I tried to reproduce the technique described by Goto, that should be used in openblas, but it doesn't really give an advantage.
---
I think it's better to stop focusing on an implementation of BLAS for now and continue with the factorization.
We will worry about which BLAS to use later on.
However, I do need to implement the partial dportf, to do partial factorization of the frontal matrices.
I also need to check if implementing dpotrf using native BLAS gives the same results as using the native Lapack.
---

*****
11 Mar 2024
I implemented dpotrf and dpotrf2.
A bit surprisingly, the C implementations that call level 2 and level 3 BLAS are faster than the native lapack library.
The advantage is around 20%, sometimes a bit more.
It's a it weird, because the performance of Lapack should come completely from BLAS, so using the same BLAS should yield the same times.
Anyway, this is a good sign, because it means that I can implement the partial dense factorization, and add regularizaiton, without penalizing the performance.
If my dpotrf was substantially slower than the native one, it would be a problem to use my own version with regularization.
I now want to try to see how much faster the packed version is.
---
Implementing the packed version has some tricky parts.
I had to find a way to pack the matrix. I will have to undo this at the end.
I had to understand how to use the BLAS functions on the packed matrices. Everything is transpose.
I has to make a local unpacked copy of the diagonal blocks and un-unpack it.
Something is not working still.
I will have to go through each step with a small matrix to see what's happening.
---

*****
12 Mar 2024
The packed version now works.
It is faster than both Lapack and the version that I made of dpotrf. However, the packing and unpacking operations can take a long time.
For now, I will ignore the packed version and use the normal one. 
In the future, I need to understand if the matrices can be produced already packed and if the forward/backsolves can be done in the packed format.
This would bring a decent advantage for large dense matrices.
---
I need to make all these code do the partial factorization.
I tried using a right looking approach in dpotrf, but this slows down the code quite a bit.
I wonder if it is faster to use a left looking approach, and at the end update the Schur complement just one.
---
Yes, using the left looking is quite faster, by a factor 3-3.5 for large matrices.
I tried to compute the number of operations that are required when doing a partial factorization with respect to the full factorization:
if only a fraction p\in[0,1] of the columns are eliminated, out of n, then the number of operations is asymptotic to
(1/3*n^3) * p*(p^2-3p+3)
where the first part (1/3*n^3) is the cost of the full factorization and p*(p^2-3p+3) is the reduction coefficient.
The times that I measure using the left and right looking approach seem to more or less follow this rule.
---

*****
13 Mar 2024
I realized that the left-looking partial factorization can be implemented using two separate arrays for the first k columns and the remaining Schur complement.
This is because the update of the Schur complement is done once at the very end, with a single BLAS call to dsyrk.
One can still use the same array for both, simply providing the correct starting location of the Schur complement block and the correct leading dimension.
I can still use restrict, because there is no aliasing happening between the two arrays.
Doing like this is useful in managing the frontal matrices.
The first k columns will be stored until the end, or copied and immediately destroyed. 
The remaining Schur complement will be kept until the parent is considered and then destroyed.
The two blocks have different lifespan within the code.
Storing them in separate arrays should make it easier to deal with this.
---

*****
14 Mar 2024
I added a function to check that the symbolic factorization is correct.
It creates a dense version of the original matrix and factorizes it using Lapack, with random entries.
It then checks that the nonzeros happen where the symbolic factorization predicts.
With random entries, numerical cancellation is highly unlikely.
If the matrix is too large, the check is not done.
---

*****
18 Mar 2024
I think it's better to avoid overcomplicationg the analyze phase for now.
I will keep the double permutation of the matrix for now.
I will keep the slow row and col counts, without using the skeleton matrix.
If needed, I will come back to these later.
---
Added computation of number of operations required, as the sum of the square of the number of nonzero entries in each column.
Added the print of the summary of the symbolic object.
---

*****
19 Mar 2024
I moved the functions that do the partial factorization into a single file PartialFact.
I also changed them so that they can do both Cholesky and LDL. 
The problem with LDL is that there are no BLAS functions with intermediate diagonal scaling.
To compute A*D*A', for a generic A and a diagonal D: 
- if D is spd, then A*D*A' = (A * D^1/2) * (D^1/2 * A') and dsyrk can be used;
- if D is generic, then I have to make a local copy of A, multiply it with D and use dgemm.
This is slower because it requires intermediate copies, but also because dgemm fills the full matrix, rather than just one of the triangles, like dsyrk.
Indeed, there is almost a factor 2 of difference when using the indefinite code instead of the positive definite.
dgemm may be more optimized than dsyrk, but it still does a more complicated operation.
I need to figure out if there is a faster way of dealing with the indefinite case.
---
To make the copies, I used dcopy.
I read in some forum that this may be less efficient than memcpy sometimes. However, I expect dcopy to be highly optimized, so it shouldn't make any difference if it is properly implemented.
I will stick to dcopy for now.
---
I should also use blocks when building the Schur complement remaining after the partial factorization.
This would use more BLAS calls, but less memory, as only a smaller copy would be needed.
However, this may be considerably slower, similarly to how the right looking approach was.
---

*****
20 Mar 2024
I did a time profile of the positive definite and indefinite dense factorizations, to see where the difference is, for a matrix of size 20000, with 10% of columns eliminated.

     Time profile     Indefinite        Pos-definite          Ratio
        Time copy       0.004962            0.000000            inf
 Time diag update       0.002041            0.002848           0.72
   Time diag fact       0.000515            0.000298           1.73
 Time update cols       0.373991            0.389060           0.96
  Time schur copy       0.043392            0.000000            inf
Time update schur       3.969590            1.965485           2.02

The copies required by the indefinite code do not take too much time.
Updating the diagonal blocks take the same time. Factorizing the diagonal blocks takes twice as much, but this is a very small number anyway.
Updating the block of columns takes the same time.
Most of the time lost is in updating the Schur complement.
I have two ideas to try:
- using blocks when updating the Schur complement
- splitting the copies between positive and negative pivots, scaling the columns with sqrt(|Ajj|) and using dsyrk, i.e.,
  the product A * D * A^T, where D is diagonal with 1 or -1 on the diagonal, can be written as
    \sum_i D_i A_i A_i^T = \sum_{i: Di=1} A_iA_i^T - \sum_{i: Di=-1} A_iA_i^T.
  In this way, I can use 2 calle to dsyrk instead of a call to dgemm.
I think that the factor 2 that arises when updating the Schur complement comes from the fact that dgemm updates the full matrix and not just the triangle.
Using dsyrk should solve this, taking some extra time for the copies.
---
Indeed, splitting into positive and negative and using dsyrk solves the problem. 

     Time profile     Indefinite        Pos-definite          Ratio
        Time copy       0.005025            0.000000            inf
 Time diag update       0.002093            0.002313           0.90
   Time diag fact       0.000510            0.000297           1.72
 Time update cols       0.376947            0.377971           1.00
  Time schur copy       0.035899            0.000000            inf
Time update schur       1.982756            1.980476           1.00

The update of the Schur complement now takes the same time.
The copies can be expensive, if many columns are eliminated (k=n or close).
Hopefully, this is not the case with the frontal matrices, as I expect only a small number of columns to be eliminated.
The update of the diagonal block is cheaper in the indefinite case probably because dgemm is better optimized than dsyrk, and the block is small.
I don't see a point in optimizing more the code for the factorization of the blocks. It takes twice as much, but this time is negligible.
---

*****
21 Mar 2024
I noticed that I also need relative indices to to assemble the original columns into the frontal matrix.
I now have relind_cols for that and relind_clique to assemble the generated elements.
In doing so, I apparently broke something and things were not working properly.
In particular, Metis was giving the usual problems, but worse.
I finally noticed that Metis assumes that the adjacency list of each vertex does not contain the vertex itself...
This means that I should not consider diagonal entries when building the full matrix for Metis.
I fixed this and now everything seems to work properly.
The old segfault and bus error and whatnot were all due to this. What an idiot.
---
I also noticed an error in the way the supernodes are detected. It has to do with Fortran and C indices starting from 1 and 0. 
It should be fixed now.
The issue of the first node not being detected correctly also disappeared.
---
I added a check for the relative indices of the columns.
I don't think this makes much sense, because the only way to check is to almost do the same operations as when I compute them.
I leave it there for now anyway.
---

*****
22 Mar 2024
I tested the analyze phase on the whole Netlib collection.
It works for all problems, both augmented system and normal equations.
I spotted another bug that was giving problems: I was using fsn_parent when it was -1. 
I have to remember to always check for roots when using the elimination tree.
The total times for the analyze phase only are:
normal equations, 0.49s
augmented system, 0.84s
---

*****
28 Mar 2024
I added a double transpose to sort the columns.
It's needed to assemble more efficiently the frontal matrices.
---
I started implementing the factorize phase.
It's a bit of a mess because Analyze takes the upper part and Factorize takes the lower part.
I may need to change the input to analyze.
---
The factorize phase was surprisingly easy to get working.
I'm not sure if everything is exactly correct, I still need to implement a proper check, but for now it seems to work.
---

*****
To do: